{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InitialDataCleaningAndModels",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPA43CiYcTeZgBqyPvkaHTt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trippzac/ToxicCommentClassification/blob/main/InitialDataCleaningAndModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz66GLmrRTLn"
      },
      "source": [
        "#Loading data and setting up environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8oRnNnzCfdl",
        "outputId": "35fc0ac2-5c83-4dcb-da2d-83389b81eabb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLw4zfWXCo4m",
        "outputId": "45793fa8-46a8-4e60-a1c2-e9f4e383b065"
      },
      "source": [
        "cd /content/drive/MyDrive/FourthBrain/IndependentProject/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/FourthBrain/IndependentProject\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19Y_G_7jJSbb"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#load in data sets\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "test_labels = pd.read_csv('test_labels.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp43B_8xC_ct"
      },
      "source": [
        "Let's get a preview of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGvBPK0POHmm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "e664c1b9-f6cc-4cfd-bc18-32e206cd8dcc"
      },
      "source": [
        "train.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>00025465d4725e87</td>\n",
              "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0002bcb3da6cb337</td>\n",
              "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>00031b1e95af7921</td>\n",
              "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00037261f536c51d</td>\n",
              "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>00040093b2687caa</td>\n",
              "      <td>alignment on this subject and which are contra...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  0000997932d777bf  ...             0\n",
              "1  000103f0d9cfb60f  ...             0\n",
              "2  000113f07ec002fd  ...             0\n",
              "3  0001b41b1c6bb37e  ...             0\n",
              "4  0001d958c54c6e35  ...             0\n",
              "5  00025465d4725e87  ...             0\n",
              "6  0002bcb3da6cb337  ...             0\n",
              "7  00031b1e95af7921  ...             0\n",
              "8  00037261f536c51d  ...             0\n",
              "9  00040093b2687caa  ...             0\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZo7gWnezD30"
      },
      "source": [
        "We combine the test data with its labels in order to classify it later on. We display the first 10 rows afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "cYzenmQ3xOwN",
        "outputId": "49c7d530-37f6-4b80-b431-a96b383c983c"
      },
      "source": [
        "labeled_test = pd.merge(test, test_labels, on=['id', 'id'])\n",
        "labeled_test.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00001cee341fdb12</td>\n",
              "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000247867823ef7</td>\n",
              "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00013b17ad220c46</td>\n",
              "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00017563c3f7919a</td>\n",
              "      <td>:If you have a look back at the source, the in...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00017695ad8997eb</td>\n",
              "      <td>I don't anonymously edit articles at all.</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0001ea8717f6de06</td>\n",
              "      <td>Thank you for understanding. I think very high...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>00024115d4cbde0f</td>\n",
              "      <td>Please do not add nonsense to Wikipedia. Such ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>000247e83dcc1211</td>\n",
              "      <td>:Dear god this site is horrible.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00025358d4737918</td>\n",
              "      <td>\" \\n Only a fool can believe in such numbers. ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>00026d1092fe71cc</td>\n",
              "      <td>== Double Redirects == \\n\\n When fixing double...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  00001cee341fdb12  ...            -1\n",
              "1  0000247867823ef7  ...            -1\n",
              "2  00013b17ad220c46  ...            -1\n",
              "3  00017563c3f7919a  ...            -1\n",
              "4  00017695ad8997eb  ...            -1\n",
              "5  0001ea8717f6de06  ...             0\n",
              "6  00024115d4cbde0f  ...            -1\n",
              "7  000247e83dcc1211  ...             0\n",
              "8  00025358d4737918  ...            -1\n",
              "9  00026d1092fe71cc  ...            -1\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwhZgJwFzccd"
      },
      "source": [
        "We notice that there are many rows with -1 as the labels. These were not used in the scoring of the Kaggle competition, and we dispense of them since they are not labeled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "BzSHDvaExXfq",
        "outputId": "8d63b9dd-91b6-4068-edef-dd5e299ba979"
      },
      "source": [
        "reduced_test = labeled_test[(labeled_test['toxic'] != -1) & \n",
        "                            (labeled_test['severe_toxic'] != -1) & \n",
        "                            (labeled_test['obscene'] != -1) & \n",
        "                            (labeled_test['threat'] != -1) & \n",
        "                            (labeled_test['insult'] != -1) & \n",
        "                            (labeled_test['identity_hate'] != -1)].reset_index().iloc[:,1:]\n",
        "reduced_test.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0001ea8717f6de06</td>\n",
              "      <td>Thank you for understanding. I think very high...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000247e83dcc1211</td>\n",
              "      <td>:Dear god this site is horrible.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0002f87b16116a7f</td>\n",
              "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0003e1cccfd5a40a</td>\n",
              "      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00059ace3e3e9a53</td>\n",
              "      <td>\" \\n\\n == Before adding a new product to the l...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>000663aff0fffc80</td>\n",
              "      <td>this other one from 1897</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>000689dd34e20979</td>\n",
              "      <td>== Reason for banning throwing == \\n\\n This ar...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>000844b52dee5f3f</td>\n",
              "      <td>|blocked]] from editing Wikipedia.   |</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00091c35fa9d0465</td>\n",
              "      <td>== Arabs are committing genocide in Iraq, but ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>000968ce11f5ee34</td>\n",
              "      <td>Please stop. If you continue to vandalize Wiki...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  0001ea8717f6de06  ...             0\n",
              "1  000247e83dcc1211  ...             0\n",
              "2  0002f87b16116a7f  ...             0\n",
              "3  0003e1cccfd5a40a  ...             0\n",
              "4  00059ace3e3e9a53  ...             0\n",
              "5  000663aff0fffc80  ...             0\n",
              "6  000689dd34e20979  ...             0\n",
              "7  000844b52dee5f3f  ...             0\n",
              "8  00091c35fa9d0465  ...             0\n",
              "9  000968ce11f5ee34  ...             0\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YNdHPB82XGL"
      },
      "source": [
        "We now determine what proportion of comments in both the training and test sets positive examples for each category of toxicity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYmnXUfu2qFG",
        "outputId": "16888703-8f10-4025-df73-c44c3ee370f8"
      },
      "source": [
        "print('Training set comments that are toxic by category:\\n',\n",
        "      '='*80,'\\n', pd.DataFrame({'Proportion': np.mean(train.iloc[:,2:],axis = 0),\n",
        "                                 'Number': np.sum(train.iloc[:,2:], axis=0)}),\n",
        "      '\\n\\n\\n', sep='')\n",
        "print('Test set comments that are toxic by category:\\n',\n",
        "      '='*80,'\\n', pd.DataFrame({'Proportion': np.mean(reduced_test.iloc[:,2:],axis = 0),\n",
        "                                 'Number': np.sum(reduced_test.iloc[:,2:], axis=0)}),\n",
        "      '\\n', sep='')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set comments that are toxic by category:\n",
            "================================================================================\n",
            "               Proportion  Number\n",
            "toxic            0.095844   15294\n",
            "severe_toxic     0.009996    1595\n",
            "obscene          0.052948    8449\n",
            "threat           0.002996     478\n",
            "insult           0.049364    7877\n",
            "identity_hate    0.008805    1405\n",
            "\n",
            "\n",
            "\n",
            "Test set comments that are toxic by category:\n",
            "================================================================================\n",
            "               Proportion  Number\n",
            "toxic            0.095189    6090\n",
            "severe_toxic     0.005736     367\n",
            "obscene          0.057692    3691\n",
            "threat           0.003298     211\n",
            "insult           0.053565    3427\n",
            "identity_hate    0.011129     712\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx8GUIfS3jhr"
      },
      "source": [
        "It appears that the proportions are similar in each category for the training and test sets. However, the data is not balanced, so we will use weighting when training later on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxnOvs1kRabV"
      },
      "source": [
        "#Cleaning data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VliS7_3rMSmU"
      },
      "source": [
        "First, we define a function to clean-up the comments and return a list of words in the comment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8N8cO39Rl4t"
      },
      "source": [
        "'''\n",
        "Input\n",
        "---------\n",
        "comment: string\n",
        "\n",
        "Output\n",
        "---------\n",
        "string: words from comment are converted to lowercase and stripped of most \n",
        "non-alphabetic characters with empty words deleted, then returned as a single\n",
        "string with space separators\n",
        "\n",
        "Note(s)\n",
        "---------\n",
        "N/A\n",
        "'''\n",
        "def clean_string(comment):\n",
        "  #convert to lowercase\n",
        "  comment = comment.lower()\n",
        "  #split into array of words\n",
        "  words = comment.split()\n",
        "  #iterate over all words, strip them of unneeded characters, and add to string\n",
        "  #to be returned\n",
        "  to_return = ''\n",
        "  for i in range(len(words)):\n",
        "    #get rid of leading or trailing characters\n",
        "    words[i] = words[i].strip('~`!@#$%^&*()_-+=\\|[{]};:\\'\\\",<.>/?/*0123456789')\n",
        "    #check if the resulting word is empty\n",
        "    if words[i] != '':\n",
        "      #add to string if not (with trailing empty space)\n",
        "      to_return += words[i] + ' '\n",
        "  return to_return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ELFiuPWMlgD"
      },
      "source": [
        "Now, we create a copy of our training data in which we replace comment_text with the simplified string of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXwltbFdMxBT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "f090df6d-149f-448b-e7f2-757e85178ba0"
      },
      "source": [
        "mod_train = train.copy()\n",
        "mod_train['comment_text'] = mod_train['comment_text'].apply(clean_string)\n",
        "mod_train.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>explanation why the edits made under my userna...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>d'aww he matches this background colour i'm se...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>hey man i'm really not trying to edit war it's...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>more i can't make any real suggestions on impr...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>you sir are my hero any chance you remember wh...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>00025465d4725e87</td>\n",
              "      <td>congratulations from me as well use the tools ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0002bcb3da6cb337</td>\n",
              "      <td>cocksucker before you piss around on my work</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>00031b1e95af7921</td>\n",
              "      <td>your vandalism to the matt shirvington article...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00037261f536c51d</td>\n",
              "      <td>sorry if the word nonsense was offensive to yo...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>00040093b2687caa</td>\n",
              "      <td>alignment on this subject and which are contra...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  0000997932d777bf  ...             0\n",
              "1  000103f0d9cfb60f  ...             0\n",
              "2  000113f07ec002fd  ...             0\n",
              "3  0001b41b1c6bb37e  ...             0\n",
              "4  0001d958c54c6e35  ...             0\n",
              "5  00025465d4725e87  ...             0\n",
              "6  0002bcb3da6cb337  ...             0\n",
              "7  00031b1e95af7921  ...             0\n",
              "8  00037261f536c51d  ...             0\n",
              "9  00040093b2687caa  ...             0\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG40lCR8NHFk"
      },
      "source": [
        "We now apply the same cleaning to the test data and check it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "UMe3GlAHNKWo",
        "outputId": "ca430ab7-b41a-4cf7-af79-91d5ac6f9763"
      },
      "source": [
        "mod_test = reduced_test.copy()\n",
        "mod_test['comment_text'] = mod_test['comment_text'].apply(clean_string)\n",
        "mod_test.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0001ea8717f6de06</td>\n",
              "      <td>thank you for understanding i think very highl...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000247e83dcc1211</td>\n",
              "      <td>dear god this site is horrible</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0002f87b16116a7f</td>\n",
              "      <td>somebody will invariably try to add religion r...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0003e1cccfd5a40a</td>\n",
              "      <td>it says it right there that it is a type the t...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00059ace3e3e9a53</td>\n",
              "      <td>before adding a new product to the list make s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>000663aff0fffc80</td>\n",
              "      <td>this other one from</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>000689dd34e20979</td>\n",
              "      <td>reason for banning throwing this article needs...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>000844b52dee5f3f</td>\n",
              "      <td>blocked from editing wikipedia</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00091c35fa9d0465</td>\n",
              "      <td>arabs are committing genocide in iraq but no p...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>000968ce11f5ee34</td>\n",
              "      <td>please stop if you continue to vandalize wikip...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  0001ea8717f6de06  ...             0\n",
              "1  000247e83dcc1211  ...             0\n",
              "2  0002f87b16116a7f  ...             0\n",
              "3  0003e1cccfd5a40a  ...             0\n",
              "4  00059ace3e3e9a53  ...             0\n",
              "5  000663aff0fffc80  ...             0\n",
              "6  000689dd34e20979  ...             0\n",
              "7  000844b52dee5f3f  ...             0\n",
              "8  00091c35fa9d0465  ...             0\n",
              "9  000968ce11f5ee34  ...             0\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OkYsAc9Zq58"
      },
      "source": [
        "#Vectorizing and Modeling\n",
        "\n",
        "We begin by importing necessary libraries for representing our data and creating a vector representation of our data. In this version, we begin with TfidfVectorizer from sklearn. We will simply input the modified training data, let it create the vector representation, and then use it to transform our test data as well in order to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfsVaOFBZ7zC"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#instantiate vectorizer with stop_words to ignore commonly occuring words with \n",
        "#little value to the comment\n",
        "vectorizer = TfidfVectorizer(max_df=0.5, stop_words='english')\n",
        "#fit vectorizer to training data\n",
        "X_train = vectorizer.fit_transform(mod_train['comment_text'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weKzAcSNkuuR"
      },
      "source": [
        "Now, we transform the test data using our fitted model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm8xwpNQk3dj"
      },
      "source": [
        "X_test = vectorizer.transform(mod_test['comment_text'].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2k87kSClSWA"
      },
      "source": [
        "We check to see what the shapes of the resulting matrices are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5JY9GoUlD0A",
        "outputId": "12a4ed73-bf7f-46f2-ed5d-87b64d1dac34"
      },
      "source": [
        "print('Training data now has the shape: ', X_train.shape)\n",
        "print('Test data now has the shape: ', X_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data now has the shape:  (159571, 181325)\n",
            "Test data now has the shape:  (63978, 181325)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c89E9i3Yvi56"
      },
      "source": [
        "##Logistic Regression\n",
        "We start with the most basic classification model, namely logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5UfTfmCv_fe"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#instantiate logistic regression with a fixed random state and balanced class\n",
        "#weights because of unbalanced data set\n",
        "log_reg = LogisticRegression(random_state=0, class_weight='balanced', max_iter=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlMtO5EYFZHk"
      },
      "source": [
        "Get train and test targets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mW_ty4yFX5e"
      },
      "source": [
        "y_train = mod_train.iloc[:,2:].values\n",
        "y_test = mod_test.iloc[:,2:].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKSMYEjSDW_X"
      },
      "source": [
        "For each target, we run a basic logistic regression and output the classification report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRmfPKEALuw5",
        "outputId": "21ab33bb-92b8-4345-b5e1-4d986c199e25"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print('Logistic Regression Predictions\\n', '='*80, '\\n\\n', sep='')\n",
        "y_pred = np.zeros(y_test.shape)\n",
        "for i in range(y_train.shape[1]):\n",
        "  print('Classification statistics for ', train.columns[2+i], ' comments\\n',\n",
        "        '-'*80, sep='')\n",
        "  model = log_reg.fit(X_train, y_train[:,i])\n",
        "  y_pred[:,i] = model.predict(X_test)\n",
        "  print(classification_report(y_test[:,i], y_pred[:,i]), '\\n\\n')\n",
        "print('Total classification statistics\\n', '='*80, '\\n', '='*80, sep='')\n",
        "print(classification_report(y_test.ravel(), y_pred.ravel()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression Predictions\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Classification statistics for toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.87      0.93     57888\n",
            "           1       0.43      0.91      0.59      6090\n",
            "\n",
            "    accuracy                           0.88     63978\n",
            "   macro avg       0.71      0.89      0.76     63978\n",
            "weighted avg       0.94      0.88      0.90     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for severe_toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98     63611\n",
            "           1       0.14      0.91      0.24       367\n",
            "\n",
            "    accuracy                           0.97     63978\n",
            "   macro avg       0.57      0.94      0.61     63978\n",
            "weighted avg       0.99      0.97      0.98     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for obscene comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.94      0.96     60287\n",
            "           1       0.46      0.88      0.61      3691\n",
            "\n",
            "    accuracy                           0.93     63978\n",
            "   macro avg       0.73      0.91      0.79     63978\n",
            "weighted avg       0.96      0.93      0.94     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for threat comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99     63767\n",
            "           1       0.18      0.82      0.30       211\n",
            "\n",
            "    accuracy                           0.99     63978\n",
            "   macro avg       0.59      0.90      0.65     63978\n",
            "weighted avg       1.00      0.99      0.99     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for insult comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.96     60551\n",
            "           1       0.39      0.87      0.54      3427\n",
            "\n",
            "    accuracy                           0.92     63978\n",
            "   macro avg       0.69      0.90      0.75     63978\n",
            "weighted avg       0.96      0.92      0.93     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for identity_hate comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.97      0.98     63266\n",
            "           1       0.23      0.85      0.37       712\n",
            "\n",
            "    accuracy                           0.97     63978\n",
            "   macro avg       0.62      0.91      0.68     63978\n",
            "weighted avg       0.99      0.97      0.98     63978\n",
            " \n",
            "\n",
            "\n",
            "Total classification statistics\n",
            "================================================================================\n",
            "================================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.94      0.97    369370\n",
            "           1       0.38      0.89      0.54     14498\n",
            "\n",
            "    accuracy                           0.94    383868\n",
            "   macro avg       0.69      0.92      0.75    383868\n",
            "weighted avg       0.97      0.94      0.95    383868\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1N_xr0VnvdQ"
      },
      "source": [
        "In the Kaggle competition, scoring was based on the mean column-wise area under the receiver operating characteristic curve, so we will display this metric for each of our models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zq1WLBBmWI8",
        "outputId": "c3592935-66d3-45a7-a6dc-822bf31aabad"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "print('ROC AUC score for logistic regression:', roc_auc_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROC AUC score for logistic regression: 0.9083097374636147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btQjoPtVoY8w"
      },
      "source": [
        "The score is 0.9083097374636147. To put this into perspective, there were 4539 contestants, and the score for 1st place (on the public leaderboard) was 0.98901, while 0.98692 was sufficient for a bronze. The current models I am creating will serve as a baseline to compare to future iterations of this project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhDS9ok_PkHv"
      },
      "source": [
        "##SVM model\n",
        "\n",
        "We now try to make a basic SVM model using LinearSVC from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2vftx8uQVF8"
      },
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "#we again balance our weights\n",
        "svm_estimator = LinearSVC(random_state=0, class_weight='balanced', max_iter=1250)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QurdEgpIQrMC"
      },
      "source": [
        "We again fit this estimator to each type of toxicity and print classification statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WGikWETQfnR",
        "outputId": "841475ce-f4bd-4e1c-bb3b-ce291deac57f"
      },
      "source": [
        "print('SVM Model Predictions\\n', '='*80, '\\n\\n', sep='')\n",
        "y_pred = np.zeros(y_test.shape)\n",
        "for i in range(y_train.shape[1]):\n",
        "  print('Classification statistics for ', train.columns[2+i], ' comments\\n',\n",
        "        '-'*80, sep='')\n",
        "  model = svm_estimator.fit(X_train, y_train[:,i])\n",
        "  y_pred[:,i] = model.predict(X_test)\n",
        "  print(classification_report(y_test[:,i], y_pred[:,i]), '\\n\\n')\n",
        "print('Total classification statistics\\n', '='*80, '\\n', '='*80, sep='')\n",
        "print(classification_report(y_test.ravel(), y_pred.ravel()))\n",
        "print('\\n\\nROC AUC score for SVM:', roc_auc_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM Model Predictions\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Classification statistics for toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.89      0.94     57888\n",
            "           1       0.46      0.88      0.60      6090\n",
            "\n",
            "    accuracy                           0.89     63978\n",
            "   macro avg       0.72      0.88      0.77     63978\n",
            "weighted avg       0.94      0.89      0.90     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for severe_toxic comments\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99     63611\n",
            "           1       0.17      0.75      0.27       367\n",
            "\n",
            "    accuracy                           0.98     63978\n",
            "   macro avg       0.58      0.86      0.63     63978\n",
            "weighted avg       0.99      0.98      0.98     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for obscene comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.95      0.97     60287\n",
            "           1       0.52      0.83      0.63      3691\n",
            "\n",
            "    accuracy                           0.95     63978\n",
            "   macro avg       0.75      0.89      0.80     63978\n",
            "weighted avg       0.96      0.95      0.95     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for threat comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     63767\n",
            "           1       0.30      0.61      0.41       211\n",
            "\n",
            "    accuracy                           0.99     63978\n",
            "   macro avg       0.65      0.80      0.70     63978\n",
            "weighted avg       1.00      0.99      1.00     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for insult comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.95      0.97     60551\n",
            "           1       0.44      0.77      0.56      3427\n",
            "\n",
            "    accuracy                           0.94     63978\n",
            "   macro avg       0.71      0.86      0.76     63978\n",
            "weighted avg       0.96      0.94      0.94     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for identity_hate comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99     63266\n",
            "           1       0.34      0.65      0.45       712\n",
            "\n",
            "    accuracy                           0.98     63978\n",
            "   macro avg       0.67      0.82      0.72     63978\n",
            "weighted avg       0.99      0.98      0.98     63978\n",
            " \n",
            "\n",
            "\n",
            "Total classification statistics\n",
            "================================================================================\n",
            "================================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.98    369370\n",
            "           1       0.44      0.82      0.57     14498\n",
            "\n",
            "    accuracy                           0.95    383868\n",
            "   macro avg       0.72      0.89      0.78    383868\n",
            "weighted avg       0.97      0.95      0.96    383868\n",
            "\n",
            "\n",
            "\n",
            "ROC AUC score for SVM: 0.8522568171983279\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EH83HOgKuy5S"
      },
      "source": [
        "The score here is 0.8522568171983279.\n",
        "\n",
        "##Dimensionality Reduction\n",
        "\n",
        "In order to apply other methods in a reasonable amount of time, we will need to apply dimensionality reduction. To do so, we will use TruncatedSVD from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNygFxrvu-t6"
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "#first, we create our transformers, starting with 10 components\n",
        "svd = TruncatedSVD(n_components=10)\n",
        "#we fit the model on X_train and transform both X_train and X_test\n",
        "X_train_red = svd.fit_transform(X_train)\n",
        "X_test_red = svd.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJQVGS-xv5C_"
      },
      "source": [
        "We check to make sure that the dimensions have been reduced. We have started with 10 components due to trial and error in training the following two methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr0DGe19v8Vb",
        "outputId": "34313137-08fb-4f54-8092-59d4a4c9b034"
      },
      "source": [
        "print('Dimensions of training data:', X_train_red.shape)\n",
        "print('Dimensions of test data:', X_test_red.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimensions of training data: (159571, 10)\n",
            "Dimensions of test data: (63978, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l52itRgLie1O"
      },
      "source": [
        "Let's get a measure of how much information is retained by this reduced data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr7Y4I88im1g",
        "outputId": "34d2f9bf-a0e0-42da-c2f6-106ae046cb01"
      },
      "source": [
        "print('Explained Variance:', svd.explained_variance_ratio_.sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Explained Variance: 0.03195950919679924\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCOMgpGaTmWc"
      },
      "source": [
        "##Gradient Boosted Trees\n",
        "\n",
        "We now try gradient boosted trees and measure their performance in a similar manner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6suTrM4LTpIy",
        "outputId": "99fd6a84-2f06-43bd-a5f7-6c40599e929d"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "#instiate estimator with fixed random state\n",
        "grad_boost_est = GradientBoostingClassifier(random_state = 0)\n",
        "\n",
        "print('Gradient Boosted Trees Predictions\\n', '='*80, '\\n\\n', sep='')\n",
        "y_pred = np.zeros(y_test.shape)\n",
        "for i in range(y_train.shape[1]):\n",
        "  print('Classification statistics for ', train.columns[2+i], ' comments\\n',\n",
        "        '-'*80, sep='')\n",
        "  model = grad_boost_est.fit(X_train_red, y_train[:,i])\n",
        "  y_pred[:,i] = model.predict(X_test_red)\n",
        "  print(classification_report(y_test[:,i], y_pred[:,i]), '\\n\\n')\n",
        "print('Total classification statistics\\n', '='*80, '\\n', '='*80, sep='')\n",
        "print(classification_report(y_test.ravel(), y_pred.ravel()))\n",
        "print('\\n\\nROC AUC score for GBT:', roc_auc_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient Boosted Trees Predictions\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Classification statistics for toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.98      0.96     57888\n",
            "           1       0.70      0.41      0.52      6090\n",
            "\n",
            "    accuracy                           0.93     63978\n",
            "   macro avg       0.82      0.70      0.74     63978\n",
            "weighted avg       0.92      0.93      0.92     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for severe_toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     63611\n",
            "           1       0.21      0.17      0.19       367\n",
            "\n",
            "    accuracy                           0.99     63978\n",
            "   macro avg       0.60      0.59      0.59     63978\n",
            "weighted avg       0.99      0.99      0.99     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for obscene comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98     60287\n",
            "           1       0.73      0.44      0.55      3691\n",
            "\n",
            "    accuracy                           0.96     63978\n",
            "   macro avg       0.85      0.71      0.76     63978\n",
            "weighted avg       0.95      0.96      0.95     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for threat comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     63767\n",
            "           1       0.02      0.00      0.01       211\n",
            "\n",
            "    accuracy                           1.00     63978\n",
            "   macro avg       0.51      0.50      0.50     63978\n",
            "weighted avg       0.99      1.00      0.99     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for insult comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.98     60551\n",
            "           1       0.63      0.36      0.46      3427\n",
            "\n",
            "    accuracy                           0.95     63978\n",
            "   macro avg       0.80      0.68      0.72     63978\n",
            "weighted avg       0.95      0.95      0.95     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for identity_hate comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99     63266\n",
            "           1       0.21      0.01      0.02       712\n",
            "\n",
            "    accuracy                           0.99     63978\n",
            "   macro avg       0.60      0.50      0.51     63978\n",
            "weighted avg       0.98      0.99      0.98     63978\n",
            " \n",
            "\n",
            "\n",
            "Total classification statistics\n",
            "================================================================================\n",
            "================================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98    369370\n",
            "           1       0.67      0.38      0.48     14498\n",
            "\n",
            "    accuracy                           0.97    383868\n",
            "   macro avg       0.82      0.68      0.73    383868\n",
            "weighted avg       0.96      0.97      0.97    383868\n",
            "\n",
            "\n",
            "\n",
            "ROC AUC score for GBT: 0.612971532491109\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Did1AzMRBmGI"
      },
      "source": [
        "The score here is 0.612971532491109, which is signficantly lower than for LinearRegression and SVM. The dimensions were reduced, but given that it took 9 minutes (compared to about a minute on LinearRegression with the whole dataset), it may be worth spending time optimizing a previous method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBgosjVOrvAP"
      },
      "source": [
        "##Random Forest Classifier\n",
        "\n",
        "We now train the last our simple models, namely a Random Forest Classifier. We again use sklearn to create our estimator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jnk7zc5sFw-"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rand_for_est = RandomForestClassifier(random_state=0, max_depth=50, n_estimators=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3dXd2E5sQXS"
      },
      "source": [
        "We now fit the model as we did above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFKjXDK9sGIU",
        "outputId": "4ba725ff-b4a2-478b-bbb3-55bf034caaa0"
      },
      "source": [
        "print('Random Forest Predictions\\n', '='*80, '\\n\\n', sep='')\n",
        "y_pred = np.zeros(y_test.shape)\n",
        "for i in range(y_train.shape[1]):\n",
        "  print('Classification statistics for ', train.columns[2+i], ' comments\\n',\n",
        "        '-'*80, sep='')\n",
        "  model = rand_for_est.fit(X_train, y_train[:,i])\n",
        "  y_pred[:,i] = model.predict(X_test)\n",
        "  print(classification_report(y_test[:,i], y_pred[:,i]), '\\n\\n')\n",
        "print('Total classification statistics\\n', '='*80, '\\n', '='*80, sep='')\n",
        "print(classification_report(y_test.ravel(), y_pred.ravel()))\n",
        "print('\\n\\nROC AUC score for SVM:', roc_auc_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Forest Predictions\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Classification statistics for toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      1.00      0.95     57888\n",
            "           1       0.97      0.01      0.03      6090\n",
            "\n",
            "    accuracy                           0.91     63978\n",
            "   macro avg       0.94      0.51      0.49     63978\n",
            "weighted avg       0.91      0.91      0.86     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for severe_toxic comments\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      1.00     63611\n",
            "           1       0.00      0.00      0.00       367\n",
            "\n",
            "    accuracy                           0.99     63978\n",
            "   macro avg       0.50      0.50      0.50     63978\n",
            "weighted avg       0.99      0.99      0.99     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for obscene comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      1.00      0.97     60287\n",
            "           1       1.00      0.01      0.01      3691\n",
            "\n",
            "    accuracy                           0.94     63978\n",
            "   macro avg       0.97      0.50      0.49     63978\n",
            "weighted avg       0.95      0.94      0.92     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for threat comments\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     63767\n",
            "           1       0.00      0.00      0.00       211\n",
            "\n",
            "    accuracy                           1.00     63978\n",
            "   macro avg       0.50      0.50      0.50     63978\n",
            "weighted avg       0.99      1.00      1.00     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for insult comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97     60551\n",
            "           1       1.00      0.00      0.01      3427\n",
            "\n",
            "    accuracy                           0.95     63978\n",
            "   macro avg       0.97      0.50      0.49     63978\n",
            "weighted avg       0.95      0.95      0.92     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for identity_hate comments\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99     63266\n",
            "           1       0.00      0.00      0.00       712\n",
            "\n",
            "    accuracy                           0.99     63978\n",
            "   macro avg       0.49      0.50      0.50     63978\n",
            "weighted avg       0.98      0.99      0.98     63978\n",
            " \n",
            "\n",
            "\n",
            "Total classification statistics\n",
            "================================================================================\n",
            "================================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98    369370\n",
            "           1       0.98      0.01      0.02     14498\n",
            "\n",
            "    accuracy                           0.96    383868\n",
            "   macro avg       0.97      0.50      0.50    383868\n",
            "weighted avg       0.96      0.96      0.94    383868\n",
            "\n",
            "\n",
            "\n",
            "ROC AUC score for SVM: 0.5019752870662554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxNOfgBHCZlt"
      },
      "source": [
        "The score is 0.5019752870662554 after about 9 minutes of fitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gK5NkdAEiWs"
      },
      "source": [
        "##Using reduced data for logistic regression and SVM\n",
        "\n",
        "We now see how logistic regression and SVM run on reduced data to see how it compares to running it on the whole data set. We will now use 100 dimensions for the reduced data, which is recommended for LSA in the TruncatedSVM documentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugxQEt5aE398",
        "outputId": "962f43b7-c254-49b5-fc8f-d3a8fdcd8c17"
      },
      "source": [
        "#first, we create our transformers, starting with 10 components\n",
        "svd = TruncatedSVD(n_components=100)\n",
        "#we fit the model on X_train and transform both X_train and X_test\n",
        "X_train_red = svd.fit_transform(X_train)\n",
        "X_test_red = svd.transform(X_test)\n",
        "print('Sizes of reduced data sets:', X_train_red.shape, ',', X_test_red.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sizes of reduced data sets: (159571, 100) , (63978, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9-wDl9ijUfC"
      },
      "source": [
        "Let's again measure how much variance is explained by this reduction in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODgQzbsCjXdo",
        "outputId": "d584a6e4-2d21-49d8-b071-af1ea46cd4e4"
      },
      "source": [
        "print('Explained variance:', svd.explained_variance_.sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Explained variance: 0.12212808179083937\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WyrIgKhj4NQ"
      },
      "source": [
        "This says that about 12% of the variance of the data is still explained, so we likely will increase the number of components in future iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeIDcoC7FRU5",
        "outputId": "90c8a550-fa68-4633-d330-d96755374375"
      },
      "source": [
        "print('Logistic Regression Predictions Using Reduced Data\\n', '='*80, '\\n\\n', sep='')\n",
        "y_pred = np.zeros(y_test.shape)\n",
        "for i in range(y_train.shape[1]):\n",
        "  print('Classification statistics for ', train.columns[2+i], ' comments\\n',\n",
        "        '-'*80, sep='')\n",
        "  model = log_reg.fit(X_train_red, y_train[:,i])\n",
        "  y_pred[:,i] = model.predict(X_test_red)\n",
        "  print(classification_report(y_test[:,i], y_pred[:,i]), '\\n\\n')\n",
        "print('Total classification statistics\\n', '='*80, '\\n', '='*80, sep='')\n",
        "print(classification_report(y_test.ravel(), y_pred.ravel()))\n",
        "print('\\n\\nROC AUC score for logistic regression:', roc_auc_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression Predictions Using Reduced Data\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Classification statistics for toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.85      0.91     57888\n",
            "           1       0.38      0.85      0.52      6090\n",
            "\n",
            "    accuracy                           0.85     63978\n",
            "   macro avg       0.68      0.85      0.72     63978\n",
            "weighted avg       0.92      0.85      0.88     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for severe_toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96     63611\n",
            "           1       0.07      0.94      0.13       367\n",
            "\n",
            "    accuracy                           0.93     63978\n",
            "   macro avg       0.53      0.93      0.55     63978\n",
            "weighted avg       0.99      0.93      0.96     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for obscene comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     60287\n",
            "           1       0.38      0.82      0.52      3691\n",
            "\n",
            "    accuracy                           0.91     63978\n",
            "   macro avg       0.69      0.87      0.74     63978\n",
            "weighted avg       0.95      0.91      0.93     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for threat comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.80      0.89     63767\n",
            "           1       0.01      0.91      0.03       211\n",
            "\n",
            "    accuracy                           0.80     63978\n",
            "   macro avg       0.51      0.85      0.46     63978\n",
            "weighted avg       1.00      0.80      0.89     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for insult comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.89      0.94     60551\n",
            "           1       0.29      0.82      0.43      3427\n",
            "\n",
            "    accuracy                           0.88     63978\n",
            "   macro avg       0.64      0.86      0.68     63978\n",
            "weighted avg       0.95      0.88      0.91     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for identity_hate comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.81      0.89     63266\n",
            "           1       0.05      0.88      0.09       712\n",
            "\n",
            "    accuracy                           0.81     63978\n",
            "   macro avg       0.52      0.84      0.49     63978\n",
            "weighted avg       0.99      0.81      0.88     63978\n",
            " \n",
            "\n",
            "\n",
            "Total classification statistics\n",
            "================================================================================\n",
            "================================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.86      0.92    369370\n",
            "           1       0.20      0.84      0.32     14498\n",
            "\n",
            "    accuracy                           0.86    383868\n",
            "   macro avg       0.59      0.85      0.62    383868\n",
            "weighted avg       0.96      0.86      0.90    383868\n",
            "\n",
            "\n",
            "\n",
            "ROC AUC score for logistic regression: 0.8678501055627228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5tM1EdpFrfU"
      },
      "source": [
        "The score for logistic regression on reduced data is 0.8678501055627228, which is about 4% worse than on the entire data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6HeoLJ5F0Lj",
        "outputId": "4b493c98-ae0f-408e-f655-1816d2f03f69"
      },
      "source": [
        "print('SVM Model Predictions\\n', '='*80, '\\n\\n', sep='')\n",
        "y_pred = np.zeros(y_test.shape)\n",
        "for i in range(y_train.shape[1]):\n",
        "  print('Classification statistics for ', train.columns[2+i], ' comments\\n',\n",
        "        '-'*80, sep='')\n",
        "  model = svm_estimator.fit(X_train_red, y_train[:,i])\n",
        "  y_pred[:,i] = model.predict(X_test_red)\n",
        "  print(classification_report(y_test[:,i], y_pred[:,i]), '\\n\\n')\n",
        "print('Total classification statistics\\n', '='*80, '\\n', '='*80, sep='')\n",
        "print(classification_report(y_test.ravel(), y_pred.ravel()))\n",
        "print('\\n\\nROC AUC score for SVM:', roc_auc_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM Model Predictions\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Classification statistics for toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.87      0.92     57888\n",
            "           1       0.40      0.84      0.54      6090\n",
            "\n",
            "    accuracy                           0.86     63978\n",
            "   macro avg       0.69      0.85      0.73     63978\n",
            "weighted avg       0.93      0.86      0.88     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for severe_toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96     63611\n",
            "           1       0.07      0.92      0.13       367\n",
            "\n",
            "    accuracy                           0.93     63978\n",
            "   macro avg       0.53      0.93      0.55     63978\n",
            "weighted avg       0.99      0.93      0.96     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for obscene comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     60287\n",
            "           1       0.40      0.82      0.53      3691\n",
            "\n",
            "    accuracy                           0.92     63978\n",
            "   macro avg       0.69      0.87      0.74     63978\n",
            "weighted avg       0.95      0.92      0.93     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for threat comments\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.78      0.88     63767\n",
            "           1       0.01      0.90      0.03       211\n",
            "\n",
            "    accuracy                           0.78     63978\n",
            "   macro avg       0.51      0.84      0.45     63978\n",
            "weighted avg       1.00      0.78      0.88     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for insult comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.89      0.94     60551\n",
            "           1       0.30      0.81      0.43      3427\n",
            "\n",
            "    accuracy                           0.89     63978\n",
            "   macro avg       0.64      0.85      0.69     63978\n",
            "weighted avg       0.95      0.89      0.91     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for identity_hate comments\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.79      0.88     63266\n",
            "           1       0.05      0.88      0.09       712\n",
            "\n",
            "    accuracy                           0.80     63978\n",
            "   macro avg       0.52      0.84      0.49     63978\n",
            "weighted avg       0.99      0.80      0.88     63978\n",
            " \n",
            "\n",
            "\n",
            "Total classification statistics\n",
            "================================================================================\n",
            "================================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.86      0.92    369370\n",
            "           1       0.19      0.83      0.31     14498\n",
            "\n",
            "    accuracy                           0.86    383868\n",
            "   macro avg       0.59      0.85      0.62    383868\n",
            "weighted avg       0.96      0.86      0.90    383868\n",
            "\n",
            "\n",
            "\n",
            "ROC AUC score for SVM: 0.8634345482710714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgrIJQcaHnWn"
      },
      "source": [
        "The score here is 0.8634345482710714, which is a bit better than SVM performed on the original data.\n",
        "\n",
        "We try transforming our data one more time to see how the performance is with 1000 components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crjMr1l3mJt1"
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "#first, we create our transformers, starting with 10 components\n",
        "svd = TruncatedSVD(n_components=1000)\n",
        "#we fit the model on X_train and transform both X_train and X_test\n",
        "X_train_red = svd.fit_transform(X_train)\n",
        "X_test_red = svd.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsKBkATvmPK9",
        "outputId": "22818f52-76af-4fa7-c052-232c3c5517b9"
      },
      "source": [
        "print('Explained variance:', svd.explained_variance_.sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Explained variance: 0.37908675784413576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVT6zC5WoAvA"
      },
      "source": [
        "Now, 37.9% of the variance is explained. Let's see how linear regression and SVM do."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LINOlAC5oN-g",
        "outputId": "91ae02f5-fc44-4a49-cfe4-df6e53ba9a3b"
      },
      "source": [
        "print('Logistic Regression Predictions Using 1000 Components\\n', '='*80, '\\n\\n', sep='')\n",
        "y_pred = np.zeros(y_test.shape)\n",
        "for i in range(y_train.shape[1]):\n",
        "  print('Classification statistics for ', train.columns[2+i], ' comments\\n',\n",
        "        '-'*80, sep='')\n",
        "  model = log_reg.fit(X_train_red, y_train[:,i])\n",
        "  y_pred[:,i] = model.predict(X_test_red)\n",
        "  print(classification_report(y_test[:,i], y_pred[:,i]), '\\n\\n')\n",
        "print('Total classification statistics\\n', '='*80, '\\n', '='*80, sep='')\n",
        "print(classification_report(y_test.ravel(), y_pred.ravel()))\n",
        "print('\\n\\nROC AUC score for logistic regression (1000 components):', roc_auc_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression Predictions Using 1000 Components\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Classification statistics for toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.86      0.92     57888\n",
            "           1       0.39      0.89      0.54      6090\n",
            "\n",
            "    accuracy                           0.86     63978\n",
            "   macro avg       0.69      0.87      0.73     63978\n",
            "weighted avg       0.93      0.86      0.88     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for severe_toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96     63611\n",
            "           1       0.07      0.95      0.14       367\n",
            "\n",
            "    accuracy                           0.93     63978\n",
            "   macro avg       0.54      0.94      0.55     63978\n",
            "weighted avg       0.99      0.93      0.96     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for obscene comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.91      0.95     60287\n",
            "           1       0.38      0.87      0.53      3691\n",
            "\n",
            "    accuracy                           0.91     63978\n",
            "   macro avg       0.69      0.89      0.74     63978\n",
            "weighted avg       0.96      0.91      0.93     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for threat comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.97     63767\n",
            "           1       0.05      0.91      0.10       211\n",
            "\n",
            "    accuracy                           0.95     63978\n",
            "   macro avg       0.53      0.93      0.54     63978\n",
            "weighted avg       1.00      0.95      0.97     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for insult comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.89      0.94     60551\n",
            "           1       0.31      0.87      0.45      3427\n",
            "\n",
            "    accuracy                           0.89     63978\n",
            "   macro avg       0.65      0.88      0.69     63978\n",
            "weighted avg       0.95      0.89      0.91     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for identity_hate comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.90      0.95     63266\n",
            "           1       0.09      0.88      0.16       712\n",
            "\n",
            "    accuracy                           0.90     63978\n",
            "   macro avg       0.54      0.89      0.55     63978\n",
            "weighted avg       0.99      0.90      0.94     63978\n",
            " \n",
            "\n",
            "\n",
            "Total classification statistics\n",
            "================================================================================\n",
            "================================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.91      0.95    369370\n",
            "           1       0.27      0.88      0.41     14498\n",
            "\n",
            "    accuracy                           0.91    383868\n",
            "   macro avg       0.63      0.89      0.68    383868\n",
            "weighted avg       0.97      0.91      0.93    383868\n",
            "\n",
            "\n",
            "\n",
            "ROC AUC score for logistic regression (1000 components): 0.8999087667929487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zs9pQbKgo-1v"
      },
      "source": [
        "This is much closer to our original performance for logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OUdkXxBppCmC",
        "outputId": "f37424fd-543d-4456-a099-a5d1c964286f"
      },
      "source": [
        "print('SVM Model Predictions (1000 components)\\n', '='*80, '\\n\\n', sep='')\n",
        "y_pred = np.zeros(y_test.shape)\n",
        "for i in range(y_train.shape[1]):\n",
        "  print('Classification statistics for ', train.columns[2+i], ' comments\\n',\n",
        "        '-'*80, sep='')\n",
        "  model = svm_estimator.fit(X_train_red, y_train[:,i])\n",
        "  y_pred[:,i] = model.predict(X_test_red)\n",
        "  print(classification_report(y_test[:,i], y_pred[:,i]), '\\n\\n')\n",
        "print('Total classification statistics\\n', '='*80, '\\n', '='*80, sep='')\n",
        "print(classification_report(y_test.ravel(), y_pred.ravel()))\n",
        "print('\\n\\nROC AUC score for SVM (1000 components):', roc_auc_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM Model Predictions (1000 components)\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Classification statistics for toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.85      0.92     57888\n",
            "           1       0.39      0.89      0.54      6090\n",
            "\n",
            "    accuracy                           0.86     63978\n",
            "   macro avg       0.69      0.87      0.73     63978\n",
            "weighted avg       0.93      0.86      0.88     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for severe_toxic comments\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96     63611\n",
            "           1       0.07      0.92      0.12       367\n",
            "\n",
            "    accuracy                           0.93     63978\n",
            "   macro avg       0.53      0.92      0.54     63978\n",
            "weighted avg       0.99      0.93      0.96     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for obscene comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.91      0.95     60287\n",
            "           1       0.38      0.87      0.53      3691\n",
            "\n",
            "    accuracy                           0.91     63978\n",
            "   macro avg       0.69      0.89      0.74     63978\n",
            "weighted avg       0.96      0.91      0.93     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for threat comments\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.97     63767\n",
            "           1       0.05      0.88      0.10       211\n",
            "\n",
            "    accuracy                           0.95     63978\n",
            "   macro avg       0.53      0.91      0.54     63978\n",
            "weighted avg       1.00      0.95      0.97     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for insult comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.89      0.94     60551\n",
            "           1       0.30      0.87      0.45      3427\n",
            "\n",
            "    accuracy                           0.89     63978\n",
            "   macro avg       0.65      0.88      0.69     63978\n",
            "weighted avg       0.95      0.89      0.91     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for identity_hate comments\n",
            "--------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.90      0.94     63266\n",
            "           1       0.09      0.87      0.16       712\n",
            "\n",
            "    accuracy                           0.90     63978\n",
            "   macro avg       0.54      0.88      0.55     63978\n",
            "weighted avg       0.99      0.90      0.94     63978\n",
            " \n",
            "\n",
            "\n",
            "Total classification statistics\n",
            "================================================================================\n",
            "================================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.91      0.95    369370\n",
            "           1       0.27      0.88      0.41     14498\n",
            "\n",
            "    accuracy                           0.90    383868\n",
            "   macro avg       0.63      0.89      0.68    383868\n",
            "weighted avg       0.97      0.90      0.93    383868\n",
            "\n",
            "\n",
            "\n",
            "ROC AUC score for SVM (1000 components): 0.8929973297971223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hv9zFnDfs_lV"
      },
      "source": [
        "In this case, we get a score of 0.8929973297971223, which is an improvement over previous models using SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWsIYpZ1KBZ0"
      },
      "source": [
        "#To do\n",
        "\n",
        "\n",
        "\n",
        "1. Improve preprocessing: View examples that are being misclassified and see if there are particular types of misspellings that are not being recognized (such as abbreviations, use of @ and other symbols, and so on). This may also involve adding a similarity metric to my words and/or adding stemming.\n",
        "2. Create visualizations near the beginning to explore the data more in-depth.\n",
        "3. Add a deep learning model. \n",
        "\n"
      ]
    }
  ]
}