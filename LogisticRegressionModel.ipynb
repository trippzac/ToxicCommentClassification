{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogisticRegressionModel",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMdVDSJWXqpnesuyZ2OHElF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trippzac/ToxicCommentClassification/blob/LogisticRegressionModel/LogisticRegressionModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz66GLmrRTLn"
      },
      "source": [
        "#Loading data and setting up environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8oRnNnzCfdl",
        "outputId": "2294dde6-5a43-42c4-a97f-7f743ecef588"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLw4zfWXCo4m",
        "outputId": "af6b74ac-dc13-47d2-9233-c49fe07765a4"
      },
      "source": [
        "cd /content/drive/MyDrive/FourthBrain/IndependentProject/"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/FourthBrain/IndependentProject\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19Y_G_7jJSbb"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#load in data sets\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "test_labels = pd.read_csv('test_labels.csv')"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp43B_8xC_ct"
      },
      "source": [
        "Let's get a preview of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGvBPK0POHmm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "929a264e-08a2-41e5-af91-30f15edb5bbd"
      },
      "source": [
        "train.head(10)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>00025465d4725e87</td>\n",
              "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0002bcb3da6cb337</td>\n",
              "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>00031b1e95af7921</td>\n",
              "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00037261f536c51d</td>\n",
              "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>00040093b2687caa</td>\n",
              "      <td>alignment on this subject and which are contra...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  0000997932d777bf  ...             0\n",
              "1  000103f0d9cfb60f  ...             0\n",
              "2  000113f07ec002fd  ...             0\n",
              "3  0001b41b1c6bb37e  ...             0\n",
              "4  0001d958c54c6e35  ...             0\n",
              "5  00025465d4725e87  ...             0\n",
              "6  0002bcb3da6cb337  ...             0\n",
              "7  00031b1e95af7921  ...             0\n",
              "8  00037261f536c51d  ...             0\n",
              "9  00040093b2687caa  ...             0\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZo7gWnezD30"
      },
      "source": [
        "We combine the test data with its labels in order to classify it later on. We display the first 10 rows afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "cYzenmQ3xOwN",
        "outputId": "3a988828-15a4-47c9-ef63-7239ec663b1a"
      },
      "source": [
        "labeled_test = pd.merge(test, test_labels, on=['id', 'id'])\n",
        "labeled_test.head(10)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00001cee341fdb12</td>\n",
              "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000247867823ef7</td>\n",
              "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00013b17ad220c46</td>\n",
              "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00017563c3f7919a</td>\n",
              "      <td>:If you have a look back at the source, the in...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00017695ad8997eb</td>\n",
              "      <td>I don't anonymously edit articles at all.</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0001ea8717f6de06</td>\n",
              "      <td>Thank you for understanding. I think very high...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>00024115d4cbde0f</td>\n",
              "      <td>Please do not add nonsense to Wikipedia. Such ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>000247e83dcc1211</td>\n",
              "      <td>:Dear god this site is horrible.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00025358d4737918</td>\n",
              "      <td>\" \\n Only a fool can believe in such numbers. ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>00026d1092fe71cc</td>\n",
              "      <td>== Double Redirects == \\n\\n When fixing double...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  00001cee341fdb12  ...            -1\n",
              "1  0000247867823ef7  ...            -1\n",
              "2  00013b17ad220c46  ...            -1\n",
              "3  00017563c3f7919a  ...            -1\n",
              "4  00017695ad8997eb  ...            -1\n",
              "5  0001ea8717f6de06  ...             0\n",
              "6  00024115d4cbde0f  ...            -1\n",
              "7  000247e83dcc1211  ...             0\n",
              "8  00025358d4737918  ...            -1\n",
              "9  00026d1092fe71cc  ...            -1\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwhZgJwFzccd"
      },
      "source": [
        "We notice that there are many rows with -1 as the labels. These were not used in the scoring of the Kaggle competition, and we dispense of them since they are not labeled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "BzSHDvaExXfq",
        "outputId": "f528370b-05f3-4416-e335-1ac5d2955666"
      },
      "source": [
        "reduced_test = labeled_test[(labeled_test['toxic'] != -1) & \n",
        "                            (labeled_test['severe_toxic'] != -1) & \n",
        "                            (labeled_test['obscene'] != -1) & \n",
        "                            (labeled_test['threat'] != -1) & \n",
        "                            (labeled_test['insult'] != -1) & \n",
        "                            (labeled_test['identity_hate'] != -1)].reset_index().iloc[:,1:]\n",
        "reduced_test.head(10)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0001ea8717f6de06</td>\n",
              "      <td>Thank you for understanding. I think very high...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000247e83dcc1211</td>\n",
              "      <td>:Dear god this site is horrible.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0002f87b16116a7f</td>\n",
              "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0003e1cccfd5a40a</td>\n",
              "      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00059ace3e3e9a53</td>\n",
              "      <td>\" \\n\\n == Before adding a new product to the l...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>000663aff0fffc80</td>\n",
              "      <td>this other one from 1897</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>000689dd34e20979</td>\n",
              "      <td>== Reason for banning throwing == \\n\\n This ar...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>000844b52dee5f3f</td>\n",
              "      <td>|blocked]] from editing Wikipedia.   |</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00091c35fa9d0465</td>\n",
              "      <td>== Arabs are committing genocide in Iraq, but ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>000968ce11f5ee34</td>\n",
              "      <td>Please stop. If you continue to vandalize Wiki...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  0001ea8717f6de06  ...             0\n",
              "1  000247e83dcc1211  ...             0\n",
              "2  0002f87b16116a7f  ...             0\n",
              "3  0003e1cccfd5a40a  ...             0\n",
              "4  00059ace3e3e9a53  ...             0\n",
              "5  000663aff0fffc80  ...             0\n",
              "6  000689dd34e20979  ...             0\n",
              "7  000844b52dee5f3f  ...             0\n",
              "8  00091c35fa9d0465  ...             0\n",
              "9  000968ce11f5ee34  ...             0\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YNdHPB82XGL"
      },
      "source": [
        "We now determine what proportion of comments in both the training and test sets positive examples for each category of toxicity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYmnXUfu2qFG",
        "outputId": "f1913062-5f42-42a0-d656-ef45207e89f1"
      },
      "source": [
        "print('Training set comments that are toxic by category:\\n',\n",
        "      '='*80,'\\n', pd.DataFrame({'Proportion': np.mean(train.iloc[:,2:],axis = 0),\n",
        "                                 'Number': np.sum(train.iloc[:,2:], axis=0)}),\n",
        "      '\\n\\n\\n', sep='')\n",
        "print('Test set comments that are toxic by category:\\n',\n",
        "      '='*80,'\\n', pd.DataFrame({'Proportion': np.mean(reduced_test.iloc[:,2:],axis = 0),\n",
        "                                 'Number': np.sum(reduced_test.iloc[:,2:], axis=0)}),\n",
        "      '\\n', sep='')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training set comments that are toxic by category:\n",
            "================================================================================\n",
            "               Proportion  Number\n",
            "toxic            0.095844   15294\n",
            "severe_toxic     0.009996    1595\n",
            "obscene          0.052948    8449\n",
            "threat           0.002996     478\n",
            "insult           0.049364    7877\n",
            "identity_hate    0.008805    1405\n",
            "\n",
            "\n",
            "\n",
            "Test set comments that are toxic by category:\n",
            "================================================================================\n",
            "               Proportion  Number\n",
            "toxic            0.095189    6090\n",
            "severe_toxic     0.005736     367\n",
            "obscene          0.057692    3691\n",
            "threat           0.003298     211\n",
            "insult           0.053565    3427\n",
            "identity_hate    0.011129     712\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx8GUIfS3jhr"
      },
      "source": [
        "It appears that the proportions are similar in each category for the training and test sets. However, the data is not balanced, so we will use weighting when training later on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxnOvs1kRabV"
      },
      "source": [
        "#Cleaning data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VliS7_3rMSmU"
      },
      "source": [
        "First, we define a function to clean-up the comments and return a list of words in the comment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8N8cO39Rl4t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d44af85-9861-4fc2-9f9c-965f8aed5659"
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "'''\n",
        "Input\n",
        "---------\n",
        "comment: string\n",
        "\n",
        "Output\n",
        "---------\n",
        "string: words from comment are converted to lowercase, stripped of most \n",
        "non-alphabetic characters, lemmatized, discarded if less than 3\n",
        "characters, and returned as a whitespace separated string\n",
        "\n",
        "Note(s)\n",
        "---------\n",
        "N/A\n",
        "'''\n",
        "def clean_string(comment):\n",
        "  #get list of stop words to exclude from data set\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  #convert to lowercase\n",
        "  comment = comment.lower()\n",
        "  #split into array of words\n",
        "  words = comment.split()\n",
        "  #strip words of leading or trailing characters\n",
        "  words = [word.strip('~`!@#$%^&*()_-+=\\|[{]};:\\'\\\",<.>/?/*0123456789') for word in words]\n",
        "  #lemmatize words\n",
        "  words = [WordNetLemmatizer().lemmatize(word) for word in words]\n",
        "  #get rid of words less than 3 characters and words in stopwords\n",
        "  #returns string separated by whitespace\n",
        "  to_return = ''\n",
        "  for word in words:\n",
        "    if len(word) > 2 and word not in stop_words:\n",
        "      to_return += word + ' '\n",
        "  return to_return"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiejvCqQjyey"
      },
      "source": [
        "Now, we edit our comments in the training and test sets via the above function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MWb4D1DkEw4"
      },
      "source": [
        "#create copies to make it easier to edit later\n",
        "mod_train = train.copy()\n",
        "mod_test = reduced_test.copy()\n",
        "\n",
        "#apply clean_string to the comments\n",
        "mod_train['comment_text'] = mod_train['comment_text'].apply(clean_string)\n",
        "mod_test['comment_text'] = mod_test['comment_text'].apply(clean_string)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47aTDx-qksGa"
      },
      "source": [
        "Let's view some of the edited comments from each set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zLaHQQMkxSr",
        "outputId": "ac2528df-e802-4f45-ebf5-154abd623b14"
      },
      "source": [
        "print('Modified training set:\\n', mod_train['comment_text'].head(10), '\\n\\n')\n",
        "print('Modified test set:\\n', mod_test['comment_text'].head(10), '\\n\\n')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Modified training set:\n",
            " 0    explanation edits made username hardcore metal...\n",
            "1    d'aww match background colour i'm seemingly st...\n",
            "2    hey man i'm really trying edit war guy constan...\n",
            "3    can't make real suggestion improvement wondere...\n",
            "4                sir hero chance remember page that's \n",
            "5              congratulation well use tool well talk \n",
            "6                         cocksucker piss around work \n",
            "7    vandalism matt shirvington article reverted pl...\n",
            "8    sorry word nonsense offensive anyway i'm inten...\n",
            "9                alignment subject contrary dulithgow \n",
            "Name: comment_text, dtype: object \n",
            "\n",
            "\n",
            "Modified test set:\n",
            " 0    thank understanding think highly would revert ...\n",
            "1                              dear god site horrible \n",
            "2    somebody invariably try add religion really me...\n",
            "3    say right type type institution needed case th...\n",
            "4    adding new product list make sure relevant add...\n",
            "5                                                 one \n",
            "6    reason banning throwing article need section t...\n",
            "7                           blocked editing wikipedia \n",
            "8    arab committing genocide iraq protest europe m...\n",
            "9    please stop continue vandalize wikipedia homos...\n",
            "Name: comment_text, dtype: object \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OkYsAc9Zq58"
      },
      "source": [
        "#Vectorizing and Modeling\n",
        "\n",
        "We begin by importing necessary libraries for representing our data and creating a vector representation of our data. In this version, we begin with TfidfVectorizer from sklearn. We will simply input the modified training data, let it create the vector representation, and then use it to transform our test data as well in order to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfsVaOFBZ7zC"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#instantiate vectorizer with limit on number of features\n",
        "vectorizer = TfidfVectorizer(max_features = 100000)\n",
        "#fit vectorizer to training data\n",
        "X_train = vectorizer.fit_transform(mod_train['comment_text'].values)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weKzAcSNkuuR"
      },
      "source": [
        "Now, we transform the test data using our fitted model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm8xwpNQk3dj"
      },
      "source": [
        "X_test = vectorizer.transform(mod_test['comment_text'].values)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2k87kSClSWA"
      },
      "source": [
        "We check to see what the shapes of the resulting matrices are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5JY9GoUlD0A",
        "outputId": "4b3d540c-ccc5-4bda-87f4-760961624e4c"
      },
      "source": [
        "print('Training data now has the shape: ', X_train.shape)\n",
        "print('Test data now has the shape: ', X_test.shape)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data now has the shape:  (159571, 100000)\n",
            "Test data now has the shape:  (63978, 100000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c89E9i3Yvi56"
      },
      "source": [
        "##Logistic Regression\n",
        "We start with the most basic classification model, namely logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5UfTfmCv_fe"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#Instantiate logistic regression with a fixed random state and balanced class\n",
        "#weights because of unbalanced data set\n",
        "log_reg = LogisticRegression(random_state=0, class_weight='balanced', max_iter=1000)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlMtO5EYFZHk"
      },
      "source": [
        "Get train and test targets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mW_ty4yFX5e"
      },
      "source": [
        "y_train = mod_train.iloc[:,2:].values\n",
        "y_test = mod_test.iloc[:,2:].values"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKSMYEjSDW_X"
      },
      "source": [
        "For each target, we run a grid cross-validation search to find the best parameters. Currently, the two parameters being tuned are C (the regularization parameter) and the penalty being applied. Note that for ElasticNet, we are currently only using the default l1_ratio. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9FzKAUGuxmy",
        "outputId": "f820dd0d-f023-451f-8436-398ece2dfdcb"
      },
      "source": [
        "log_reg.get_params().keys()"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['C', 'class_weight', 'dual', 'fit_intercept', 'intercept_scaling', 'l1_ratio', 'max_iter', 'multi_class', 'n_jobs', 'penalty', 'random_state', 'solver', 'tol', 'verbose', 'warm_start'])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceItGvYarZyl",
        "outputId": "84385571-01cf-4e53-ef24-4de1a1af2867"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "#create a grid of parameters for C (we may optimize on penalty later if saga\n",
        "#solver will run fast enough)\n",
        "param_grid = {'C': [.03, .1, .3, 1, 3]}\n",
        "\n",
        "print('Logistic Regression Predictions\\n', '='*80, '\\n\\n', sep='')\n",
        "y_pred = np.zeros(y_test.shape)\n",
        "clf = GridSearchCV(log_reg, param_grid, scoring='roc_auc')\n",
        "for i in range(y_train.shape[1]):\n",
        "  print('Best parameters for ', train.columns[2+i], ' comments\\n',\n",
        "        '-'*80, sep='')\n",
        "  clf.fit(X_train, y_train[:,i])\n",
        "  print(clf.best_params_, '\\n\\n')\n",
        "  y_pred[:,i] = clf.predict(X_test)\n",
        "print('Overall score:', roc_auc_score(y_test, y_pred))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic Regression Predictions\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Best parameters for toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "{'C': 1} \n",
            "\n",
            "\n",
            "Best parameters for severe_toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "{'C': 0.1} \n",
            "\n",
            "\n",
            "Best parameters for obscene comments\n",
            "--------------------------------------------------------------------------------\n",
            "{'C': 1} \n",
            "\n",
            "\n",
            "Best parameters for threat comments\n",
            "--------------------------------------------------------------------------------\n",
            "{'C': 0.1} \n",
            "\n",
            "\n",
            "Best parameters for insult comments\n",
            "--------------------------------------------------------------------------------\n",
            "{'C': 1} \n",
            "\n",
            "\n",
            "Best parameters for identity_hate comments\n",
            "--------------------------------------------------------------------------------\n",
            "{'C': 0.3} \n",
            "\n",
            "\n",
            "Overall score: 0.9161478469936725\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1N_xr0VnvdQ"
      },
      "source": [
        "In the Kaggle competition, scoring was based on the mean column-wise area under the receiver operating characteristic curve, so we will display this metric for each of our models.\n",
        "\n",
        "Now, let's see if we can determine what types of errors are occuring in order data by running a classification report and then pulling some of the misclassified data and inspecting it by hand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpmyD_PCOvOv",
        "outputId": "725eddbc-7147-4101-a645-5c3e373c100f"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "for i in range(y_train.shape[1]):\n",
        "  print('Classification statistics for ', train.columns[2+i], ' comments\\n',\n",
        "        '-'*80, sep='')\n",
        "  print(classification_report(y_test[:,i], y_pred[:,i]), '\\n\\n')\n",
        "print('Total classification statistics\\n', '='*80, '\\n', '='*80, sep='')\n",
        "print(classification_report(y_test.ravel(), y_pred.ravel()))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classification statistics for toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.87      0.93     57888\n",
            "           1       0.43      0.92      0.58      6090\n",
            "\n",
            "    accuracy                           0.88     63978\n",
            "   macro avg       0.71      0.89      0.76     63978\n",
            "weighted avg       0.94      0.88      0.89     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for severe_toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.98     63611\n",
            "           1       0.10      0.93      0.19       367\n",
            "\n",
            "    accuracy                           0.95     63978\n",
            "   macro avg       0.55      0.94      0.58     63978\n",
            "weighted avg       0.99      0.95      0.97     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for obscene comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.93      0.96     60287\n",
            "           1       0.44      0.88      0.59      3691\n",
            "\n",
            "    accuracy                           0.93     63978\n",
            "   macro avg       0.72      0.91      0.78     63978\n",
            "weighted avg       0.96      0.93      0.94     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for threat comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99     63767\n",
            "           1       0.11      0.88      0.20       211\n",
            "\n",
            "    accuracy                           0.98     63978\n",
            "   macro avg       0.56      0.93      0.59     63978\n",
            "weighted avg       1.00      0.98      0.99     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for insult comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     60551\n",
            "           1       0.38      0.88      0.53      3427\n",
            "\n",
            "    accuracy                           0.92     63978\n",
            "   macro avg       0.69      0.90      0.74     63978\n",
            "weighted avg       0.96      0.92      0.93     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for identity_hate comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.96      0.98     63266\n",
            "           1       0.18      0.89      0.31       712\n",
            "\n",
            "    accuracy                           0.95     63978\n",
            "   macro avg       0.59      0.93      0.64     63978\n",
            "weighted avg       0.99      0.95      0.97     63978\n",
            " \n",
            "\n",
            "\n",
            "Total classification statistics\n",
            "================================================================================\n",
            "================================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.94      0.96    369370\n",
            "           1       0.35      0.90      0.51     14498\n",
            "\n",
            "    accuracy                           0.93    383868\n",
            "   macro avg       0.68      0.92      0.74    383868\n",
            "weighted avg       0.97      0.93      0.95    383868\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1djRvr79PX2v"
      },
      "source": [
        "It appears that the model has quite low precision, particularly for threat and severe_toxic comments. In our training data, these comments and identity_hate comments have particularly low sample sizes, so we may need to do some data engineering or use another type of algorithm to boost them.\n",
        "\n",
        "Let's first view some of the misclassified data for threat and severe_toxic comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcVvSKZEQJVb",
        "outputId": "8513afda-8af6-48e6-9933-a7db37521bc4"
      },
      "source": [
        "#first, we create a DataFrame of our test set with the predicted labels added on\n",
        "test_with_pred_labels = mod_test.copy()\n",
        "#we add the column names, along with an initial p_ for predicted\n",
        "new_cols = 'p_' + test_with_pred_labels.columns[2:]\n",
        "#add columns\n",
        "test_with_pred_labels[new_cols] = y_pred\n",
        "\n",
        "#now, we retrieve the misclassified threats and print out some of the data\n",
        "misclassified_indices_threat = (y_test[:,3] != y_pred[:,3])\n",
        "#print out examples (for best viewing)\n",
        "for i in range(20):\n",
        "  print(test_with_pred_labels.comment_text[misclassified_indices_threat].iloc[i], '\\n\\n')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "burn hell revoke talk page access  \n",
            "\n",
            "\n",
            "black mamba it.is ponious snake word kill many people king cobra kill many people india  \n",
            "\n",
            "\n",
            "trust trust checkuser work eat burn guarantee hate seriously  \n",
            "\n",
            "\n",
            "i'll  \n",
            "\n",
            "\n",
            "hell  \n",
            "\n",
            "\n",
            "drunk made gibson say really belief little freak nazi like father hope lung cancer soon  \n",
            "\n",
            "\n",
            "still shouldnt kill woman reason like christains soppose good people  \n",
            "\n",
            "\n",
            "said going longgggg one  \n",
            "\n",
            "\n",
            "think delete whole fuckin wikipedia  \n",
            "\n",
            "\n",
            "cite policy wasting fucking time cite policy i'll strike bastard better good fucking idea complaint say wasting goddamn time opinion worthless matter policy guideline source give pile steaming dogshit manner sounding nonsense bullshit understanding community whole fuck lot ruder opinion jesus christ stop polite-trolling learn fucking rule wikipedia's rules:simple/complex  \n",
            "\n",
            "\n",
            "removal band several band list may thrash metal band either black death posting band comment whether thrash genre bracket disputed desaster black destryer blackened thrash divine empire death ghoul death grimfist blackened death hatesphere melodic death helstar speed impious death kat black legion damned death loudblast death lyzanxia death messiah death morbid saint death black mortification death nevermore heavy metal one look like child bodom pestilence death rage speed red harvest industrial sex machineguns speed skrew industrial stam1na alternative progressive stigma death susperia melodic black terror melodic death torture squad death transmetal death witchery black death yyrkoon death expect many well find misplaced band add list  \n",
            "\n",
            "\n",
            "view point dark/light thinking person global warming going kill need stop showing paris hilton britney spear useless garbage fill news show global warming serious really exist extra precaution save planet delaware freeze water level return normal ice serious kiss planet goodbye we're rushing blindly future knowing driving convience time help plant tree stop brazil slashand burn beating showing farm way find offensive i'm talking exactly stop worrying american idol start growin tree two replace classic bulb new bulb politician start worrying preaching view world fall ruin die next year ozone layer rip open across whole world we'll vote worry ass election promise cheap easy attract start following gore's advice moron soaked sea capitalism car airplane ciggaretes lightbulb along energy factory kill future actually enjoy planet sorry excessive way world revolves money rule thing  \n",
            "\n",
            "\n",
            "i'm gay whack ogrish rape  \n",
            "\n",
            "\n",
            "wat heck hell wrong next time vandalize page i'm going fayetteville arkansas beating crap  \n",
            "\n",
            "\n",
            "cut bitch  \n",
            "\n",
            "\n",
            "heard at&t guy heart attack die tomorrow  \n",
            "\n",
            "\n",
            "marmilade bastard pro-assad wedge-assad die  \n",
            "\n",
            "\n",
            "threatening block reason fucking bitch  \n",
            "\n",
            "\n",
            "fuck mother fucker  \n",
            "\n",
            "\n",
            "dude...get fucking clue goth dressing black shit...u hail satian sort shit emo emo sit room day staring ceiling wondering anyone would really care fell killing instantly...i know...emo self harm cutting etc...wanting die...just like  \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JWyNUeEzWUu"
      },
      "source": [
        "Let's additionally view how these comments are being labeled in the other categories and what the most common words are in these comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 883
        },
        "id": "j_TuJ-eBzlQN",
        "outputId": "28d4c627-8347-4720-e448-edce36e6dae2"
      },
      "source": [
        "test_with_pred_labels[misclassified_indices_threat].iloc[:20,:]"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "      <th>p_toxic</th>\n",
              "      <th>p_severe_toxic</th>\n",
              "      <th>p_obscene</th>\n",
              "      <th>p_threat</th>\n",
              "      <th>p_insult</th>\n",
              "      <th>p_identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0016b94c8b20ffa6</td>\n",
              "      <td>burn hell revoke talk page access</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>00372dfdd531fc6c</td>\n",
              "      <td>black mamba it.is ponious snake word kill many...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>009ba60f01432c26</td>\n",
              "      <td>trust trust checkuser work eat burn guarantee ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>164</th>\n",
              "      <td>00ad0323d9b31f21</td>\n",
              "      <td>i'll</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>00b3813b966af7e8</td>\n",
              "      <td>hell</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>00bd66c9ef023f41</td>\n",
              "      <td>drunk made gibson say really belief little fre...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>011b51b386c60373</td>\n",
              "      <td>still shouldnt kill woman reason like christai...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>291</th>\n",
              "      <td>0123f7c07bedbf0d</td>\n",
              "      <td>said going longgggg one</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307</th>\n",
              "      <td>0137ae56816b36fa</td>\n",
              "      <td>think delete whole fuckin wikipedia</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>0173dd710621e443</td>\n",
              "      <td>cite policy wasting fucking time cite policy i...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>445</th>\n",
              "      <td>01bb6d3defb1f068</td>\n",
              "      <td>removal band several band list may thrash meta...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>516</th>\n",
              "      <td>01fc7740970dde12</td>\n",
              "      <td>view point dark/light thinking person global w...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>525</th>\n",
              "      <td>020314bca6eede81</td>\n",
              "      <td>i'm gay whack ogrish rape</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>530</th>\n",
              "      <td>0207ab2b69d1adcc</td>\n",
              "      <td>wat heck hell wrong next time vandalize page i...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>533</th>\n",
              "      <td>020aac5d42dac2dd</td>\n",
              "      <td>cut bitch</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>540</th>\n",
              "      <td>02110f3abb6d7c4b</td>\n",
              "      <td>heard at&amp;t guy heart attack die tomorrow</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>573</th>\n",
              "      <td>023ca2f6f57bd009</td>\n",
              "      <td>marmilade bastard pro-assad wedge-assad die</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>626</th>\n",
              "      <td>027ef348b4796a0d</td>\n",
              "      <td>threatening block reason fucking bitch</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>720</th>\n",
              "      <td>02e8e228dc5899ce</td>\n",
              "      <td>fuck mother fucker</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>722</th>\n",
              "      <td>02eac2931d524e13</td>\n",
              "      <td>dude...get fucking clue goth dressing black sh...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   id  ... p_identity_hate\n",
              "27   0016b94c8b20ffa6  ...             0.0\n",
              "61   00372dfdd531fc6c  ...             0.0\n",
              "148  009ba60f01432c26  ...             0.0\n",
              "164  00ad0323d9b31f21  ...             0.0\n",
              "172  00b3813b966af7e8  ...             0.0\n",
              "184  00bd66c9ef023f41  ...             1.0\n",
              "280  011b51b386c60373  ...             1.0\n",
              "291  0123f7c07bedbf0d  ...             0.0\n",
              "307  0137ae56816b36fa  ...             1.0\n",
              "375  0173dd710621e443  ...             0.0\n",
              "445  01bb6d3defb1f068  ...             1.0\n",
              "516  01fc7740970dde12  ...             0.0\n",
              "525  020314bca6eede81  ...             1.0\n",
              "530  0207ab2b69d1adcc  ...             0.0\n",
              "533  020aac5d42dac2dd  ...             1.0\n",
              "540  02110f3abb6d7c4b  ...             0.0\n",
              "573  023ca2f6f57bd009  ...             0.0\n",
              "626  027ef348b4796a0d  ...             1.0\n",
              "720  02e8e228dc5899ce  ...             1.0\n",
              "722  02eac2931d524e13  ...             0.0\n",
              "\n",
              "[20 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMKkT_3Q88kz"
      },
      "source": [
        "It appears that most errors are not due to misrecognizing words but instead not being able to distinguish between the different categories. Perhaps a sentiment analysis would improve this. The lemmatization and other minor changes to preprocessing above have added about .01 to our ROC AUC score, which is a good improvement. Currently, there is not a clear way to improve the preprocessing given that the biggest issue is overidentifying threats, severe_toxic comments, etc.\n",
        "\n",
        "Let's give a summary of some of the words that are being misclassified. We will continue to focus on the threats for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Hc5r76e9_Jx",
        "outputId": "d707af2c-1dcc-4c3e-91bc-7b0d9cf21ea2"
      },
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "#first, create list of all words in misclassified comments\n",
        "misclassified_words = []\n",
        "for word in test_with_pred_labels['comment_text']:\n",
        "  misclassified_words += word.split()\n",
        "#find a frequency distribution of the words\n",
        "fdist = FreqDist(misclassified_words)\n",
        "#print 30 most common words\n",
        "print('Word                Count\\n', '-'*80, sep='')\n",
        "for word, count in fdist.most_common(30):\n",
        "  print(word, '.'*(20-len(word)), count, sep='')\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word                Count\n",
            "--------------------------------------------------------------------------------\n",
            "article.............26976\n",
            "page................17888\n",
            "would...............11226\n",
            "one.................11096\n",
            "wikipedia...........10924\n",
            "like................10791\n",
            "please..............9657\n",
            "source..............8333\n",
            "think...............8290\n",
            "see.................7972\n",
            "also................7410\n",
            "know................7064\n",
            "i'm.................7039\n",
            "people..............6717\n",
            "time................6665\n",
            "make................5959\n",
            "fuck................5930\n",
            "use.................5821\n",
            "talk................5772\n",
            "say.................5742\n",
            "edit................5574\n",
            "may.................5534\n",
            "need................5413\n",
            "get.................5223\n",
            "name................4924\n",
            "section.............4871\n",
            "thanks..............4795\n",
            "even................4675\n",
            "doe.................4651\n",
            "good................4584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JopEVAYF0Hi"
      },
      "source": [
        "Interestingly, many of the words appear to revolve around related concepts like \"article\", \"page\", \"wikipedia\", \"source\", etc. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWsIYpZ1KBZ0"
      },
      "source": [
        "#To do\n",
        "\n",
        "\n",
        "\n",
        "1. Consider doing some data engineering to add more non-toxic examples related to wikipedia and other commonly misclassified words above.\n",
        "2. Create a deep learning model, perhaps using BERT.\n"
      ]
    }
  ]
}