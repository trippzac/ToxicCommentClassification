{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogisticRegressionModel",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMBNWeqWiba5hY93j3TOCWg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trippzac/ToxicCommentClassification/blob/main/LogisticRegressionModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz66GLmrRTLn"
      },
      "source": [
        "#Loading data and setting up environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8oRnNnzCfdl",
        "outputId": "5c994bcc-4cf2-4741-9d3f-075b6d5ae78b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLw4zfWXCo4m",
        "outputId": "fb69a110-ce13-4c55-9db6-5b908a2b1fda"
      },
      "source": [
        "cd /content/drive/MyDrive/FourthBrain/IndependentProject/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/FourthBrain/IndependentProject\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19Y_G_7jJSbb"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#load in data sets\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "test_labels = pd.read_csv('test_labels.csv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp43B_8xC_ct"
      },
      "source": [
        "Let's get a preview of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGvBPK0POHmm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "9082a471-4e07-433c-9efe-0648764024a4"
      },
      "source": [
        "train.head(10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0000997932d777bf</td>\n",
              "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000103f0d9cfb60f</td>\n",
              "      <td>D'aww! He matches this background colour I'm s...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000113f07ec002fd</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0001b41b1c6bb37e</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0001d958c54c6e35</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>00025465d4725e87</td>\n",
              "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0002bcb3da6cb337</td>\n",
              "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>00031b1e95af7921</td>\n",
              "      <td>Your vandalism to the Matt Shirvington article...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00037261f536c51d</td>\n",
              "      <td>Sorry if the word 'nonsense' was offensive to ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>00040093b2687caa</td>\n",
              "      <td>alignment on this subject and which are contra...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  0000997932d777bf  ...             0\n",
              "1  000103f0d9cfb60f  ...             0\n",
              "2  000113f07ec002fd  ...             0\n",
              "3  0001b41b1c6bb37e  ...             0\n",
              "4  0001d958c54c6e35  ...             0\n",
              "5  00025465d4725e87  ...             0\n",
              "6  0002bcb3da6cb337  ...             0\n",
              "7  00031b1e95af7921  ...             0\n",
              "8  00037261f536c51d  ...             0\n",
              "9  00040093b2687caa  ...             0\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZo7gWnezD30"
      },
      "source": [
        "We combine the test data with its labels in order to classify it later on. We display the first 10 rows afterwards."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "cYzenmQ3xOwN",
        "outputId": "a8cdbeec-05aa-4ce1-bf0a-f83825d600ca"
      },
      "source": [
        "labeled_test = pd.merge(test, test_labels, on=['id', 'id'])\n",
        "labeled_test.head(10)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00001cee341fdb12</td>\n",
              "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0000247867823ef7</td>\n",
              "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00013b17ad220c46</td>\n",
              "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>00017563c3f7919a</td>\n",
              "      <td>:If you have a look back at the source, the in...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00017695ad8997eb</td>\n",
              "      <td>I don't anonymously edit articles at all.</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0001ea8717f6de06</td>\n",
              "      <td>Thank you for understanding. I think very high...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>00024115d4cbde0f</td>\n",
              "      <td>Please do not add nonsense to Wikipedia. Such ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>000247e83dcc1211</td>\n",
              "      <td>:Dear god this site is horrible.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00025358d4737918</td>\n",
              "      <td>\" \\n Only a fool can believe in such numbers. ...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>00026d1092fe71cc</td>\n",
              "      <td>== Double Redirects == \\n\\n When fixing double...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  00001cee341fdb12  ...            -1\n",
              "1  0000247867823ef7  ...            -1\n",
              "2  00013b17ad220c46  ...            -1\n",
              "3  00017563c3f7919a  ...            -1\n",
              "4  00017695ad8997eb  ...            -1\n",
              "5  0001ea8717f6de06  ...             0\n",
              "6  00024115d4cbde0f  ...            -1\n",
              "7  000247e83dcc1211  ...             0\n",
              "8  00025358d4737918  ...            -1\n",
              "9  00026d1092fe71cc  ...            -1\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwhZgJwFzccd"
      },
      "source": [
        "We notice that there are many rows with -1 as the labels. These were not used in the scoring of the Kaggle competition, and we dispense of them since they are not labeled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "BzSHDvaExXfq",
        "outputId": "0b1aebef-ff18-4b70-b2e2-f782417ccfc7"
      },
      "source": [
        "reduced_test = labeled_test[(labeled_test['toxic'] != -1) & \n",
        "                            (labeled_test['severe_toxic'] != -1) & \n",
        "                            (labeled_test['obscene'] != -1) & \n",
        "                            (labeled_test['threat'] != -1) & \n",
        "                            (labeled_test['insult'] != -1) & \n",
        "                            (labeled_test['identity_hate'] != -1)].reset_index().iloc[:,1:]\n",
        "reduced_test.head(10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0001ea8717f6de06</td>\n",
              "      <td>Thank you for understanding. I think very high...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000247e83dcc1211</td>\n",
              "      <td>:Dear god this site is horrible.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0002f87b16116a7f</td>\n",
              "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0003e1cccfd5a40a</td>\n",
              "      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00059ace3e3e9a53</td>\n",
              "      <td>\" \\n\\n == Before adding a new product to the l...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>000663aff0fffc80</td>\n",
              "      <td>this other one from 1897</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>000689dd34e20979</td>\n",
              "      <td>== Reason for banning throwing == \\n\\n This ar...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>000844b52dee5f3f</td>\n",
              "      <td>|blocked]] from editing Wikipedia.   |</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00091c35fa9d0465</td>\n",
              "      <td>== Arabs are committing genocide in Iraq, but ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>000968ce11f5ee34</td>\n",
              "      <td>Please stop. If you continue to vandalize Wiki...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  ... identity_hate\n",
              "0  0001ea8717f6de06  ...             0\n",
              "1  000247e83dcc1211  ...             0\n",
              "2  0002f87b16116a7f  ...             0\n",
              "3  0003e1cccfd5a40a  ...             0\n",
              "4  00059ace3e3e9a53  ...             0\n",
              "5  000663aff0fffc80  ...             0\n",
              "6  000689dd34e20979  ...             0\n",
              "7  000844b52dee5f3f  ...             0\n",
              "8  00091c35fa9d0465  ...             0\n",
              "9  000968ce11f5ee34  ...             0\n",
              "\n",
              "[10 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YNdHPB82XGL"
      },
      "source": [
        "We now determine what proportion of comments in both the training and test sets positive examples for each category of toxicity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYmnXUfu2qFG",
        "outputId": "0de68491-fc8a-45b3-d45b-054599546c9b"
      },
      "source": [
        "print('Training set comments that are toxic by category:\\n',\n",
        "      '='*80,'\\n', pd.DataFrame({'Proportion': np.mean(train.iloc[:,2:],axis = 0),\n",
        "                                 'Number': np.sum(train.iloc[:,2:], axis=0)}),\n",
        "      '\\n\\n\\n', sep='')\n",
        "print('Test set comments that are toxic by category:\\n',\n",
        "      '='*80,'\\n', pd.DataFrame({'Proportion': np.mean(reduced_test.iloc[:,2:],axis = 0),\n",
        "                                 'Number': np.sum(reduced_test.iloc[:,2:], axis=0)}),\n",
        "      '\\n', sep='')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set comments that are toxic by category:\n",
            "================================================================================\n",
            "               Proportion  Number\n",
            "toxic            0.095844   15294\n",
            "severe_toxic     0.009996    1595\n",
            "obscene          0.052948    8449\n",
            "threat           0.002996     478\n",
            "insult           0.049364    7877\n",
            "identity_hate    0.008805    1405\n",
            "\n",
            "\n",
            "\n",
            "Test set comments that are toxic by category:\n",
            "================================================================================\n",
            "               Proportion  Number\n",
            "toxic            0.095189    6090\n",
            "severe_toxic     0.005736     367\n",
            "obscene          0.057692    3691\n",
            "threat           0.003298     211\n",
            "insult           0.053565    3427\n",
            "identity_hate    0.011129     712\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx8GUIfS3jhr"
      },
      "source": [
        "It appears that the proportions are similar in each category for the training and test sets. However, the data is not balanced, so we will use weighting when training later on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxnOvs1kRabV"
      },
      "source": [
        "#Cleaning data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VliS7_3rMSmU"
      },
      "source": [
        "First, we define a function to clean-up the comments and return a list of words in the comment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8N8cO39Rl4t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "982b7c22-b36a-4d81-e58d-8dc99f106f8a"
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "'''\n",
        "Input\n",
        "---------\n",
        "comment: string\n",
        "\n",
        "Output\n",
        "---------\n",
        "string: words from comment are converted to lowercase, stripped of most \n",
        "non-alphabetic characters, lemmatized, discarded if less than 3\n",
        "characters, and returned as a whitespace separated string\n",
        "\n",
        "Note(s)\n",
        "---------\n",
        "N/A\n",
        "'''\n",
        "def clean_string(comment):\n",
        "  #get list of stop words to exclude from data set\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  #convert to lowercase\n",
        "  comment = comment.lower()\n",
        "  #split into array of words\n",
        "  words = comment.split()\n",
        "  #strip words of leading or trailing characters\n",
        "  words = [word.strip('~`!@#$%^&*()_-+=\\|[{]};:\\'\\\",<.>/?/*0123456789') for word in words]\n",
        "  #lemmatize words\n",
        "  words = [WordNetLemmatizer().lemmatize(word) for word in words]\n",
        "  #get rid of words less than 3 characters and words in stopwords\n",
        "  #returns string separated by whitespace\n",
        "  to_return = ''\n",
        "  for word in words:\n",
        "    if len(word) > 2 and word not in stop_words:\n",
        "      to_return += word + ' '\n",
        "  return to_return"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiejvCqQjyey"
      },
      "source": [
        "Now, we edit our comments in the training and test sets via the above function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MWb4D1DkEw4"
      },
      "source": [
        "#create copies to make it easier to edit later\n",
        "mod_train = train.copy()\n",
        "mod_test = reduced_test.copy()\n",
        "\n",
        "#apply clean_string to the comments\n",
        "mod_train['comment_text'] = mod_train['comment_text'].apply(clean_string)\n",
        "mod_test['comment_text'] = mod_test['comment_text'].apply(clean_string)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47aTDx-qksGa"
      },
      "source": [
        "Let's view some of the edited comments from each set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zLaHQQMkxSr",
        "outputId": "6eec5830-86d2-41fd-8f9b-49620a2ab39e"
      },
      "source": [
        "print('Modified training set:\\n', mod_train['comment_text'].head(10), '\\n\\n')\n",
        "print('Modified test set:\\n', mod_test['comment_text'].head(10), '\\n\\n')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modified training set:\n",
            " 0    explanation edits made username hardcore metal...\n",
            "1    d'aww match background colour i'm seemingly st...\n",
            "2    hey man i'm really trying edit war guy constan...\n",
            "3    can't make real suggestion improvement wondere...\n",
            "4                sir hero chance remember page that's \n",
            "5              congratulation well use tool well talk \n",
            "6                         cocksucker piss around work \n",
            "7    vandalism matt shirvington article reverted pl...\n",
            "8    sorry word nonsense offensive anyway i'm inten...\n",
            "9                alignment subject contrary dulithgow \n",
            "Name: comment_text, dtype: object \n",
            "\n",
            "\n",
            "Modified test set:\n",
            " 0    thank understanding think highly would revert ...\n",
            "1                              dear god site horrible \n",
            "2    somebody invariably try add religion really me...\n",
            "3    say right type type institution needed case th...\n",
            "4    adding new product list make sure relevant add...\n",
            "5                                                 one \n",
            "6    reason banning throwing article need section t...\n",
            "7                           blocked editing wikipedia \n",
            "8    arab committing genocide iraq protest europe m...\n",
            "9    please stop continue vandalize wikipedia homos...\n",
            "Name: comment_text, dtype: object \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OkYsAc9Zq58"
      },
      "source": [
        "#Vectorizing and Modeling\n",
        "\n",
        "We begin by importing necessary libraries for representing our data and creating a vector representation of our data. In this version, we begin with TfidfVectorizer from sklearn. We will simply input the modified training data, let it create the vector representation, and then use it to transform our test data as well in order to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfsVaOFBZ7zC"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#instantiate vectorizer with limit on number of features\n",
        "vectorizer = TfidfVectorizer(max_features = 100000)\n",
        "#fit vectorizer to training data\n",
        "X_train = vectorizer.fit_transform(mod_train['comment_text'].values)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weKzAcSNkuuR"
      },
      "source": [
        "Now, we transform the test data using our fitted model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm8xwpNQk3dj"
      },
      "source": [
        "X_test = vectorizer.transform(mod_test['comment_text'].values)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2k87kSClSWA"
      },
      "source": [
        "We check to see what the shapes of the resulting matrices are."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5JY9GoUlD0A",
        "outputId": "375fd9ba-3494-46e8-9a03-1a9e22b583ce"
      },
      "source": [
        "print('Training data now has the shape: ', X_train.shape)\n",
        "print('Test data now has the shape: ', X_test.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data now has the shape:  (159571, 100000)\n",
            "Test data now has the shape:  (63978, 100000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c89E9i3Yvi56"
      },
      "source": [
        "##Logistic Regression\n",
        "We start with the most basic classification model, namely logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5UfTfmCv_fe"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#Instantiate logistic regression with a fixed random state and balanced class\n",
        "#weights because of unbalanced data set\n",
        "log_reg = LogisticRegression(random_state=0, class_weight='balanced', max_iter=1000)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlMtO5EYFZHk"
      },
      "source": [
        "Get train and test targets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mW_ty4yFX5e"
      },
      "source": [
        "y_train = mod_train.iloc[:,2:].values\n",
        "y_test = mod_test.iloc[:,2:].values"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKSMYEjSDW_X"
      },
      "source": [
        "For each target, we run a grid cross-validation search to find the best parameters. Currently, the two parameters being tuned are C (the regularization parameter) and the penalty being applied. Note that for ElasticNet, we are currently only using the default l1_ratio. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9FzKAUGuxmy",
        "outputId": "e33e31ad-17f2-4b2d-fef1-4f9df6436348"
      },
      "source": [
        "log_reg.get_params().keys()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['C', 'class_weight', 'dual', 'fit_intercept', 'intercept_scaling', 'l1_ratio', 'max_iter', 'multi_class', 'n_jobs', 'penalty', 'random_state', 'solver', 'tol', 'verbose', 'warm_start'])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceItGvYarZyl",
        "outputId": "1098441b-942f-416d-8029-344bfee30e6e"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "#create a grid of parameters for C (we may optimize on penalty later if saga\n",
        "#solver will run fast enough)\n",
        "param_grid = {'C': [.03, .1, .3, 1, 3]}\n",
        "\n",
        "print('Logistic Regression Predictions\\n', '='*80, '\\n\\n', sep='')\n",
        "y_pred = np.zeros(y_test.shape)\n",
        "clf = GridSearchCV(log_reg, param_grid, scoring='roc_auc')\n",
        "for i in range(y_train.shape[1]):\n",
        "  print('Best parameters for ', train.columns[2+i], ' comments\\n',\n",
        "        '-'*80, sep='')\n",
        "  clf.fit(X_train, y_train[:,i])\n",
        "  print(clf.best_params_, '\\n\\n')\n",
        "  y_pred[:,i] = np.reshape(clf.predict_proba(X_test)[:, clf.classes_ == 1], (y_test.shape[0],))\n",
        "print('Overall score:', roc_auc_score(y_test, y_pred))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Predictions\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Best parameters for toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "{'C': 1} \n",
            "\n",
            "\n",
            "Best parameters for severe_toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "{'C': 0.1} \n",
            "\n",
            "\n",
            "Best parameters for obscene comments\n",
            "--------------------------------------------------------------------------------\n",
            "{'C': 1} \n",
            "\n",
            "\n",
            "Best parameters for threat comments\n",
            "--------------------------------------------------------------------------------\n",
            "{'C': 0.1} \n",
            "\n",
            "\n",
            "Best parameters for insult comments\n",
            "--------------------------------------------------------------------------------\n",
            "{'C': 1} \n",
            "\n",
            "\n",
            "Best parameters for identity_hate comments\n",
            "--------------------------------------------------------------------------------\n",
            "{'C': 0.3} \n",
            "\n",
            "\n",
            "Overall score: 0.9751622534421358\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1N_xr0VnvdQ"
      },
      "source": [
        "In the Kaggle competition, scoring was based on the mean column-wise area under the receiver operating characteristic curve, so we will display this metric for each of our models.\n",
        "\n",
        "Now, let's see if we can determine what types of errors are occuring in order data by running a classification report and then pulling some of the misclassified data and inspecting it by hand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpmyD_PCOvOv",
        "outputId": "34b2759d-4b0b-4158-8ef1-565a497fc308"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "for i in range(y_train.shape[1]):\n",
        "  print('Classification statistics for ', train.columns[2+i], ' comments\\n',\n",
        "        '-'*80, sep='')\n",
        "  print(classification_report(y_test[:,i], np.round(y_pred[:,i])), '\\n\\n')\n",
        "print('Total classification statistics\\n', '='*80, '\\n', '='*80, sep='')\n",
        "print(classification_report(y_test.ravel(), np.round(y_pred.ravel())))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification statistics for toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.87      0.93     57888\n",
            "           1       0.43      0.92      0.58      6090\n",
            "\n",
            "    accuracy                           0.88     63978\n",
            "   macro avg       0.71      0.89      0.76     63978\n",
            "weighted avg       0.94      0.88      0.89     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for severe_toxic comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.95      0.98     63611\n",
            "           1       0.10      0.93      0.19       367\n",
            "\n",
            "    accuracy                           0.95     63978\n",
            "   macro avg       0.55      0.94      0.58     63978\n",
            "weighted avg       0.99      0.95      0.97     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for obscene comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.93      0.96     60287\n",
            "           1       0.44      0.88      0.59      3691\n",
            "\n",
            "    accuracy                           0.93     63978\n",
            "   macro avg       0.72      0.91      0.78     63978\n",
            "weighted avg       0.96      0.93      0.94     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for threat comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99     63767\n",
            "           1       0.11      0.88      0.20       211\n",
            "\n",
            "    accuracy                           0.98     63978\n",
            "   macro avg       0.56      0.93      0.59     63978\n",
            "weighted avg       1.00      0.98      0.99     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for insult comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.92      0.95     60551\n",
            "           1       0.38      0.88      0.53      3427\n",
            "\n",
            "    accuracy                           0.92     63978\n",
            "   macro avg       0.69      0.90      0.74     63978\n",
            "weighted avg       0.96      0.92      0.93     63978\n",
            " \n",
            "\n",
            "\n",
            "Classification statistics for identity_hate comments\n",
            "--------------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.96      0.98     63266\n",
            "           1       0.18      0.89      0.31       712\n",
            "\n",
            "    accuracy                           0.95     63978\n",
            "   macro avg       0.59      0.93      0.64     63978\n",
            "weighted avg       0.99      0.95      0.97     63978\n",
            " \n",
            "\n",
            "\n",
            "Total classification statistics\n",
            "================================================================================\n",
            "================================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.94      0.96    369370\n",
            "           1       0.35      0.90      0.51     14498\n",
            "\n",
            "    accuracy                           0.93    383868\n",
            "   macro avg       0.68      0.92      0.74    383868\n",
            "weighted avg       0.97      0.93      0.95    383868\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1djRvr79PX2v"
      },
      "source": [
        "It appears that the model has quite low precision, particularly for threat and severe_toxic comments. In our training data, these comments and identity_hate comments have particularly low sample sizes, so we may need to do some data engineering or use another type of algorithm to boost them.\n",
        "\n",
        "Let's first view some of the misclassified data for threat and severe_toxic comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcVvSKZEQJVb",
        "outputId": "95d1b66e-bff3-4750-8e9c-1e89ddc62f6d"
      },
      "source": [
        "#first, we create a DataFrame of our test set with the predicted labels added on\n",
        "test_with_pred_labels = mod_test.copy()\n",
        "#we add the column names, along with an initial p_ for predicted\n",
        "new_cols = 'p_' + test_with_pred_labels.columns[2:]\n",
        "#add columns\n",
        "test_with_pred_labels[new_cols] = y_pred\n",
        "\n",
        "#now, we retrieve the misclassified threats and print out some of the data\n",
        "misclassified_indices_threat = (y_test[:,3] != y_pred[:,3])\n",
        "#print out examples (for best viewing)\n",
        "for i in range(20):\n",
        "  print(test_with_pred_labels.comment_text[misclassified_indices_threat].iloc[i], '\\n\\n')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thank understanding think highly would revert without discussion  \n",
            "\n",
            "\n",
            "dear god site horrible  \n",
            "\n",
            "\n",
            "somebody invariably try add religion really mean way people invariably kept adding religion samuel beckett infobox bother bringing long-dead completely non-existent influence issue flailing making crap fly comparison explicit acknowledgement entire amos article personally jewish category  \n",
            "\n",
            "\n",
            "say right type type institution needed case three level suny school university center doctoral granting institution state college community college needed case clarify suny center say even binghamton university university albany state university new york stony brook university stop trying say totally right case  \n",
            "\n",
            "\n",
            "adding new product list make sure relevant adding new product list make sure wikipedia entry already proving relevance giving reader possibility read otherwise could subject deletion see article's revision history  \n",
            "\n",
            "\n",
            "one  \n",
            "\n",
            "\n",
            "reason banning throwing article need section throwing banned moment non-cricket fan seems kind arbitrary  \n",
            "\n",
            "\n",
            "blocked editing wikipedia  \n",
            "\n",
            "\n",
            "arab committing genocide iraq protest europe may europe also burn hell  \n",
            "\n",
            "\n",
            "please stop continue vandalize wikipedia homosexuality blocked editing  \n",
            "\n",
            "\n",
            "energy edited introduction previously said passive transport doe use kind energy true passive transport relies kinetic energy substance transported kinetic energy cause move around random chance cross membrane difference active transport actually cell's energy atp electrochemical gradient pump substance across membrane  \n",
            "\n",
            "\n",
            "redslash cut short source stating rok sovereign post otherwise please aknowledge place make  \n",
            "\n",
            "\n",
            "jew race get mother mention ethiopian jew testing jew prof well fact accept convert  \n",
            "\n",
            "\n",
            "ollie others think one list oldest people know long easy answer raise cutoff age purely round number full year shorter record make top top everyone tell maximum list size set threshold  \n",
            "\n",
            "\n",
            "professor manhatten project  \n",
            "\n",
            "\n",
            "sure whether notable enough mentioned article right version later open file created previous version save file format suffix msx version number intentional course besides version save file compressed format ☭共产主义万岁★  \n",
            "\n",
            "\n",
            "일이삼사오육칠팔구하고십이요 에헤헤 으허허  \n",
            "\n",
            "\n",
            "balance page one sentence basic definition word huge amount slang/profane perhaps former extended information female dog available beyond name encyclopaedia dictionary feel whoever looking definition appropiate deleted wikipedia...immediatly word used often also mean word belive majorly true much okay good meaning female dog bitch also stand name brittany fellows—preceding unsigned comment added etymology word bitch old norse bikkjuna meaning female dog unknown origin grimm derives old norse word lapp pittja oed note converse equally possible adj bitchy first seen verb meaning complain slang bitchen good attested reclaiming word bitch word bitch actually offencive american canadian english english variant bitch maintains correct definition female canine people argued bitch different used insult pig dog cow others considered profane bitch far concerned nothing reclaim bitch simply mean female canine may used pejorative descriptor doe make profanity definately say something kyle vanderweilen bitchin particular evidence woman reclaiming word bitch anyone point article etc song definitely interesting belongs actually reclaim word bitch reclaims word sinner also really understand last paragraph suck going try clear realized know mean someone point source lay argument bitch fertility patriarchy clearly need article definitely enough example even outside missy elliot cleary repeatedly reclaims word instance bitch reference fine name missy elliot got find quote rolling stone review mentioned reclaiming word fact one know might interesting add section work relates women/slurs generally within hip-hop relevant reclamation seems poison woman use epithet still strongly connotes despised trait see using word connoting querolous spiteful malicious empowering know life's bitch stem reaction prevalence tagging woman adhere certain standard femininity bitch use literary example novel handmaid's tale narrator related feeling whenever outsmarted man could almost hear calling bitch mind even husband literary reaction recited jealousy men name men outsmart word like bitch resort greater variety pejoritive epithet though specific think standard femininity referred pretext subordination sex often strive impose men traditionally power popular epithet slang connotation bitch diffuse though still commonly pernicious think bitch chiefly denunciation punish woman conform standard rather contemptuous word often used however still see point reclamati  \n",
            "\n",
            "\n",
            "redirect talk:mi vida eres  \n",
            "\n",
            "\n",
            "i'm convinced blind documented possible we'd call legally blind great vision name blind blake exaggerated moniker although proof i've got feeling legally blind likely totally blind course i've got evidence back thing  \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JWyNUeEzWUu"
      },
      "source": [
        "Let's additionally view how these comments are being labeled in the other categories and what the most common words are in these comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        },
        "id": "j_TuJ-eBzlQN",
        "outputId": "2839713c-60d7-497e-ed39-e81e559554ce"
      },
      "source": [
        "test_with_pred_labels[misclassified_indices_threat].iloc[:20,:]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>severe_toxic</th>\n",
              "      <th>obscene</th>\n",
              "      <th>threat</th>\n",
              "      <th>insult</th>\n",
              "      <th>identity_hate</th>\n",
              "      <th>p_toxic</th>\n",
              "      <th>p_severe_toxic</th>\n",
              "      <th>p_obscene</th>\n",
              "      <th>p_threat</th>\n",
              "      <th>p_insult</th>\n",
              "      <th>p_identity_hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0001ea8717f6de06</td>\n",
              "      <td>thank understanding think highly would revert ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.023539</td>\n",
              "      <td>0.023013</td>\n",
              "      <td>0.009752</td>\n",
              "      <td>0.023101</td>\n",
              "      <td>0.029696</td>\n",
              "      <td>0.015527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000247e83dcc1211</td>\n",
              "      <td>dear god site horrible</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.952607</td>\n",
              "      <td>0.125183</td>\n",
              "      <td>0.275395</td>\n",
              "      <td>0.149069</td>\n",
              "      <td>0.373076</td>\n",
              "      <td>0.081922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0002f87b16116a7f</td>\n",
              "      <td>somebody invariably try add religion really me...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.169434</td>\n",
              "      <td>0.093134</td>\n",
              "      <td>0.105610</td>\n",
              "      <td>0.070059</td>\n",
              "      <td>0.036268</td>\n",
              "      <td>0.047454</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0003e1cccfd5a40a</td>\n",
              "      <td>say right type type institution needed case th...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.023646</td>\n",
              "      <td>0.046607</td>\n",
              "      <td>0.041533</td>\n",
              "      <td>0.037857</td>\n",
              "      <td>0.012091</td>\n",
              "      <td>0.025095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>00059ace3e3e9a53</td>\n",
              "      <td>adding new product list make sure relevant add...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.012886</td>\n",
              "      <td>0.010635</td>\n",
              "      <td>0.008257</td>\n",
              "      <td>0.008475</td>\n",
              "      <td>0.003972</td>\n",
              "      <td>0.002021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>000663aff0fffc80</td>\n",
              "      <td>one</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.091462</td>\n",
              "      <td>0.046239</td>\n",
              "      <td>0.028433</td>\n",
              "      <td>0.053239</td>\n",
              "      <td>0.067842</td>\n",
              "      <td>0.041082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>000689dd34e20979</td>\n",
              "      <td>reason banning throwing article need section t...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.066946</td>\n",
              "      <td>0.030963</td>\n",
              "      <td>0.019774</td>\n",
              "      <td>0.031265</td>\n",
              "      <td>0.023990</td>\n",
              "      <td>0.024839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>000844b52dee5f3f</td>\n",
              "      <td>blocked editing wikipedia</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.124661</td>\n",
              "      <td>0.056203</td>\n",
              "      <td>0.030600</td>\n",
              "      <td>0.039864</td>\n",
              "      <td>0.052189</td>\n",
              "      <td>0.017054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>00091c35fa9d0465</td>\n",
              "      <td>arab committing genocide iraq protest europe m...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.659700</td>\n",
              "      <td>0.238634</td>\n",
              "      <td>0.243680</td>\n",
              "      <td>0.498761</td>\n",
              "      <td>0.271281</td>\n",
              "      <td>0.403402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>000968ce11f5ee34</td>\n",
              "      <td>please stop continue vandalize wikipedia homos...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.377633</td>\n",
              "      <td>0.041448</td>\n",
              "      <td>0.073898</td>\n",
              "      <td>0.089557</td>\n",
              "      <td>0.116106</td>\n",
              "      <td>0.255592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0009734200a85047</td>\n",
              "      <td>energy edited introduction previously said pas...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.055574</td>\n",
              "      <td>0.099209</td>\n",
              "      <td>0.033100</td>\n",
              "      <td>0.079547</td>\n",
              "      <td>0.036236</td>\n",
              "      <td>0.067891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>000a02d807ae0254</td>\n",
              "      <td>redslash cut short source stating rok sovereig...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.125115</td>\n",
              "      <td>0.074421</td>\n",
              "      <td>0.100180</td>\n",
              "      <td>0.133739</td>\n",
              "      <td>0.054463</td>\n",
              "      <td>0.064094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>000bafe2080bba82</td>\n",
              "      <td>jew race get mother mention ethiopian jew test...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.711745</td>\n",
              "      <td>0.446508</td>\n",
              "      <td>0.170991</td>\n",
              "      <td>0.312179</td>\n",
              "      <td>0.495687</td>\n",
              "      <td>0.990693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>000bf0a9894b2807</td>\n",
              "      <td>ollie others think one list oldest people know...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.037176</td>\n",
              "      <td>0.046663</td>\n",
              "      <td>0.028194</td>\n",
              "      <td>0.031072</td>\n",
              "      <td>0.049063</td>\n",
              "      <td>0.019010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>000c9b92318552d1</td>\n",
              "      <td>professor manhatten project</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.124935</td>\n",
              "      <td>0.153750</td>\n",
              "      <td>0.059650</td>\n",
              "      <td>0.090530</td>\n",
              "      <td>0.063552</td>\n",
              "      <td>0.038726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>000cf60dbaed8c02</td>\n",
              "      <td>sure whether notable enough mentioned article ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.018783</td>\n",
              "      <td>0.026517</td>\n",
              "      <td>0.007709</td>\n",
              "      <td>0.022002</td>\n",
              "      <td>0.006954</td>\n",
              "      <td>0.005981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>000d4f120d5a7303</td>\n",
              "      <td>일이삼사오육칠팔구하고십이요 에헤헤 으허허</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.223988</td>\n",
              "      <td>0.161466</td>\n",
              "      <td>0.103745</td>\n",
              "      <td>0.102147</td>\n",
              "      <td>0.125802</td>\n",
              "      <td>0.101262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>001068b809feee6b</td>\n",
              "      <td>balance page one sentence basic definition wor...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.957515</td>\n",
              "      <td>0.297093</td>\n",
              "      <td>0.996682</td>\n",
              "      <td>0.043390</td>\n",
              "      <td>0.962743</td>\n",
              "      <td>0.047351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0011cefc680993ba</td>\n",
              "      <td>redirect talk:mi vida eres</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.053454</td>\n",
              "      <td>0.090737</td>\n",
              "      <td>0.046461</td>\n",
              "      <td>0.044729</td>\n",
              "      <td>0.033656</td>\n",
              "      <td>0.030198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0011ef6aa33d42e6</td>\n",
              "      <td>i'm convinced blind documented possible we'd c...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.208931</td>\n",
              "      <td>0.076889</td>\n",
              "      <td>0.082877</td>\n",
              "      <td>0.054675</td>\n",
              "      <td>0.258966</td>\n",
              "      <td>0.054343</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  id  ... p_identity_hate\n",
              "0   0001ea8717f6de06  ...        0.015527\n",
              "1   000247e83dcc1211  ...        0.081922\n",
              "2   0002f87b16116a7f  ...        0.047454\n",
              "3   0003e1cccfd5a40a  ...        0.025095\n",
              "4   00059ace3e3e9a53  ...        0.002021\n",
              "5   000663aff0fffc80  ...        0.041082\n",
              "6   000689dd34e20979  ...        0.024839\n",
              "7   000844b52dee5f3f  ...        0.017054\n",
              "8   00091c35fa9d0465  ...        0.403402\n",
              "9   000968ce11f5ee34  ...        0.255592\n",
              "10  0009734200a85047  ...        0.067891\n",
              "11  000a02d807ae0254  ...        0.064094\n",
              "12  000bafe2080bba82  ...        0.990693\n",
              "13  000bf0a9894b2807  ...        0.019010\n",
              "14  000c9b92318552d1  ...        0.038726\n",
              "15  000cf60dbaed8c02  ...        0.005981\n",
              "16  000d4f120d5a7303  ...        0.101262\n",
              "17  001068b809feee6b  ...        0.047351\n",
              "18  0011cefc680993ba  ...        0.030198\n",
              "19  0011ef6aa33d42e6  ...        0.054343\n",
              "\n",
              "[20 rows x 14 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMKkT_3Q88kz"
      },
      "source": [
        "It appears that most errors are not due to misrecognizing words but instead not being able to distinguish between the different categories. Perhaps a sentiment analysis would improve this. The lemmatization and other minor changes to preprocessing above have added about .01 to our ROC AUC score, which is a good improvement. Currently, there is not a clear way to improve the preprocessing given that the biggest issue is overidentifying threats, severe_toxic comments, etc.\n",
        "\n",
        "Let's give a summary of some of the words that are being misclassified. We will continue to focus on the threats for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Hc5r76e9_Jx",
        "outputId": "fc49e297-48ef-4a92-88ed-5c627e9f93ae"
      },
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "#first, create list of all words in misclassified comments\n",
        "misclassified_words = []\n",
        "for word in test_with_pred_labels['comment_text']:\n",
        "  misclassified_words += word.split()\n",
        "#find a frequency distribution of the words\n",
        "fdist = FreqDist(misclassified_words)\n",
        "#print 30 most common words\n",
        "print('Word                Count\\n', '-'*80, sep='')\n",
        "for word, count in fdist.most_common(30):\n",
        "  print(word, '.'*(20-len(word)), count, sep='')\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word                Count\n",
            "--------------------------------------------------------------------------------\n",
            "article.............26976\n",
            "page................17888\n",
            "would...............11226\n",
            "one.................11096\n",
            "wikipedia...........10924\n",
            "like................10791\n",
            "please..............9657\n",
            "source..............8333\n",
            "think...............8290\n",
            "see.................7972\n",
            "also................7410\n",
            "know................7064\n",
            "i'm.................7039\n",
            "people..............6717\n",
            "time................6665\n",
            "make................5959\n",
            "fuck................5930\n",
            "use.................5821\n",
            "talk................5772\n",
            "say.................5742\n",
            "edit................5574\n",
            "may.................5534\n",
            "need................5413\n",
            "get.................5223\n",
            "name................4924\n",
            "section.............4871\n",
            "thanks..............4795\n",
            "even................4675\n",
            "doe.................4651\n",
            "good................4584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JopEVAYF0Hi"
      },
      "source": [
        "Interestingly, many of the words appear to revolve around related concepts like \"article\", \"page\", \"wikipedia\", \"source\", etc. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWsIYpZ1KBZ0"
      },
      "source": [
        "#To do\n",
        "\n",
        "\n",
        "\n",
        "1. Consider doing some data engineering to add more non-toxic examples related to wikipedia and other commonly misclassified words above.\n",
        "2. Create a deep learning model, perhaps using BERT.\n"
      ]
    }
  ]
}